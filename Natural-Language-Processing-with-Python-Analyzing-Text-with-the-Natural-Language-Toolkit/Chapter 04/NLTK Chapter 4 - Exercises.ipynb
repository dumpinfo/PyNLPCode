{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 4\n",
    "\n",
    "## Writing Structured Programs\n",
    "\n",
    "*The html version of this chapter in the NLTK book is available [here](https://www.nltk.org/book/ch04.html#exercises \"Ch04 Exercises\").*\n",
    "\n",
    "### 4.11   Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###### 1. \n",
    "\n",
    "☼ Find out more about sequence objects using Python's help facility. In the interpreter, type `help(str)`, `help(list)`, and `help(tuple)`. This will give you a full list of the functions supported by each type. Some functions have special names flanked with underscore; as the help documentation shows, each such function corresponds to something more familiar. For example `x.__getitem__(y)` is just a long-winded way of saying `x[y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. \n",
    "\n",
    "☼ Identify three operations that can be performed on both tuples and lists. Identify three list operations that cannot be performed on tuples. Name a context where using a list instead of a tuple generates a Python error.\n",
    "\n",
    "*Operations that can be performed on both tuples and lists:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in dir(list) if x in dir(tuple)], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Operations that can be performed on lists, but not tuples:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in dir(list) if x not in dir(tuple)], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Trying to use a list as a key in a dictionary will not work, but it's possible with a tuple:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (\"Snugglebunnies\")\n",
    "b = [\"Basselopes\"]\n",
    "\n",
    "c = {a: \"N\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saved as markdown because having a cell that throws an error will cause my notebook to go higgledy piddledy:*\n",
    "\n",
    "```\n",
    "c = {b: \"N\"}\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-17-ee5632d64820> in <module>\n",
    "----> 1 c = {b: \"N\"}\n",
    "\n",
    "TypeError: unhashable type: 'list'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is because tuples are hashable, but lists are not:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "hash(b)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-19-ad85d8b55702> in <module>\n",
    "----> 1 hash(b)\n",
    "\n",
    "TypeError: unhashable type: 'list'\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. \n",
    "\n",
    "☼ Find out how to create a tuple consisting of a single item. There are at least two ways to do this.\n",
    "\n",
    "*Add a common after a single value, or create a list with a single value, and use `tuple` to convert this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1, \n",
    "\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tuple([1])\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ Create a list `words = ['is', 'NLP', 'fun', '?']`. Use a series of assignment statements (e.g. `words[1] = words[2]`) and a temporary variable `tmp` to transform this list into the list `['NLP', 'is', 'fun', '!']`. Now do the same transformation using tuple assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['is', 'NLP', 'fun', '?']\n",
    "tmp = words[0]\n",
    "words[0] = words[1]\n",
    "words[1] = tmp\n",
    "words[3] = '!'\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['is', 'NLP', 'fun', '?']\n",
    "words[1], words[0], words[3] = words[0], words[1], \"!\"\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n",
    "\n",
    "☼ Read about the built-in comparison function `cmp`, by typing `help(cmp)`. How does it differ in behavior from the comparison operators?\n",
    "\n",
    "*`cmp` has been deprecated.  Get with the times, authors!*\n",
    "\n",
    "```\n",
    "help(cmp)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-29-6cc5f65683db> in <module>\n",
    "----> 1 help(cmp)\n",
    "\n",
    "NameError: name 'cmp' is not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. \n",
    "\n",
    "☼ Does the method for creating a sliding window of n-grams behave correctly for the two limiting cases: $n$ = 1, and $n$ = `len(sent)`?\n",
    "\n",
    "*No.  $n$ = 1 will just return __unigrams__, i.e., the individual words which comprised the sentence.  $n$ = `len(sent)` will just return the entire list:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "n = 1\n",
    "[sent[i:i+n] for i in range(len(sent)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    ">>> n = len(sent)\n",
    ">>> [sent[i:i+n] for i in range(len(sent)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "☼ We pointed out that when empty strings and empty lists occur in the condition part of an `if` clause, they evaluate to `False`. In this case, they are said to be occurring in a Boolean context. Experiment with different kind of non-Boolean expressions in Boolean contexts, and see whether they evaluate as `True` or `False`.\n",
    "\n",
    "*With the exception of 0, all numbers evaluate as True.  Even `float('Inf')`, `-float('Inf')`, and `float('NaN')`. All strings evaluate as True, except for the empty string. Empty lists and tuples will evaluate as False.  `None` evaluates as false, but `None` in a list or a tuple evaluates as True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = [0, 1, -1, float('Inf'), -float('Inf'), float('NaN'), \"0\", \"1\", \"\", [], \n",
    "         \"Fahrvergnügen\", 3.1415, None, [None], tuple([]), tuple([None])]\n",
    "\n",
    "for c in cands:\n",
    "    if c:\n",
    "        print(\"{} evaluates as True\".format(c))\n",
    "    else:\n",
    "        print(\"{} evaluates as False\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. \n",
    "\n",
    "☼ Use the inequality operators to compare strings, e.g. `'Monty' < 'Python'`. What happens when you do `'Z' < 'a'`? Try pairs of strings which have a common prefix, e.g. `'Monty' < 'Montague'`. Read up on \"lexicographical sort\" in order to understand what is going on here. Try comparing structured objects, e.g. `('Monty', 1) < ('Monty', 2)`. Does this behave as expected?\n",
    "\n",
    "*In lexicographical sort, only the first index in both items is compared.  Since 'M' comes before 'P', the rest of the string is ignored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'M' < 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'P'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uppercase letters are considered as coming 'before' lowercase ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Z' < 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*'Monty' and 'Montague' have the same first four elements.  Since 'y' comes after 'a', the comparison evaluates as `False`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'Montague'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As the first elements in both tuples are identical, the second element is compared, and one is indeed less than 2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Monty', 1) < ('Monty', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Carrying on that logic a bit further:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Monty', 1, 1, 1, 5) < ('Monty', 1, 1, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.\n",
    "\n",
    "☼ Write code that removes whitespace at the beginning and end of a string, and normalizes whitespace between words to be a single space character.\n",
    "\n",
    "* 1. do this task using `split()` and `join()`\n",
    "* 2. do this task using regular expression substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"    this    is   a really inconsistent          use  of     whitespace.      \"\n",
    "sent = sent.split()\n",
    "' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"    this    is   a really inconsistent          use  of     whitespace.      \"\n",
    "sent = re.sub(r'^\\s*|\\s*$', '', sent)\n",
    "sent = re.sub(r'\\s+', ' ', sent)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.\n",
    "\n",
    "☼ Write a program to sort words by length. Define a helper function `cmp_len` which uses the `cmp` comparison function on word lengths.\n",
    "\n",
    "*`cmp` is still deprecated, so I'll only do the first part of this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_length(text):\n",
    "    \"\"\"\n",
    "    Returns a list, sorted from shortest to longest,\n",
    "    of words in text.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [w for _, w in sorted([(len(w), w) for w in text.split()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'the words in this sentence are mostly of unique character lengths'\n",
    "\n",
    "print(sort_words_by_length(sent), end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "◑ Create a list of words and store it in a variable `sent1`. Now assign `sent2 = sent1`. Modify one of the items in `sent1` and verify that `sent2` has changed.\n",
    "\n",
    "* a. Now try the same exercise but instead assign `sent2 = sent1[:]`. Modify sent1 again and see what happens to `sent2`. Explain.\n",
    "* b. Now define `text1` to be a list of lists of strings (e.g. to represent a text consisting of multiple sentences. Now assign `text2 = text1[:]`, assign a new value to one of the words, e.g. `text1[1][1] = 'Monty'`. Check what this did to `text2`. Explain.\n",
    "* c. Load Python's `deepcopy()` function (i.e. `from copy import deepcopy`), consult its documentation, and test that it makes a fresh copy of any object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = sent1\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__ Using `[:]` causes `sent1` to be shallow copied, so changes aren't replicated in `sent2`.  I.e., it's a reference to a copy of the list, and not the original list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = sent1[:]\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ Now the change is permeated.  It appears a shallow copy of a list of lists is just a copy of the references of the lists.  I.e., if one of the original lists is changed, the changed is reflected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA = \"Mairzy doats and dozy doats and liddle lamzy divey\"\n",
    "sentB = \"A kiddley divey too, wouldn't you?\"\n",
    "\n",
    "text1 = [sentA.split(), sentB.split()]\n",
    "text2 = text1[:]\n",
    "\n",
    "text1[1][1] = 'Monty'\n",
    "\n",
    "print(text2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we explicitly make shallow copies of the lists in the list of lists, then the changes won't be replicated:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA = \"Mairzy doats and dozy doats and liddle lamzy divey\"\n",
    "sentB = \"A kiddley divey too, wouldn't you?\"\n",
    "\n",
    "text1 = [sentA.split(), sentB.split()]\n",
    "text2 = [text1[0][:], text1[1][:]]\n",
    "\n",
    "text1[1][1] = 'Monty'\n",
    "\n",
    "print(text2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = deepcopy(sent1)\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. \n",
    "\n",
    "◑ Initialize an $n$-by-$m$ list of lists of empty strings using list multiplication, e.g. `word_table = [[''] * n] * m`. What happens when you set one of its values, e.g. `word_table[1][2] = \"hello\"`? Explain why this happens. Now write an expression using `range()` to construct a list of lists, and show that it does not have this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 6, 7\n",
    "\n",
    "word_table = [[''] * n] * m\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table[1][2] = (\"hello\")\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Above we have a copy of a list containing sublists. When we make a change to one of the sublists, the copy is replicated in all of the lists which have been created by copying it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table = [['' for i in range(n)] for j in range(m)]\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table[1][2] = (\"hello\")\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this case, we're creating a brand new empty string for each of the iterations through $n$, and likewise through $m$.  As the strings are not copies of each other, changes in one are not reflected in the others.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. \n",
    "\n",
    "◑ Write code to initialize a two-dimensional array of sets called `word_vowels` and process a list of words, adding each word to `word_vowels[l][v]` where `l` is the length of the word and `v` is the number of vowels it contains.\n",
    "\n",
    "*If we follow the instructions literally and create an array of sets, both sets will come back sorted, and it is very likely that the $n$th item in the first set will not be the same as the $n$th item in the second.  I.e., a long word without many vowels would be represented towards the end of the first set, but towards the beginning of the second (and vice versa):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length_word_number_vowels(text):\n",
    "    \"\"\"\n",
    "    Returns an array of two sets.  The first set comprises\n",
    "    the lengths of the words.  The second the number of vowels.\n",
    "    For the sake of simplicity, 'y' is not counted as a vowel.\n",
    "    Results are ordered from smallest to greatest.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vowels = [set(), set()]\n",
    "\n",
    "    for t in text:\n",
    "        word_vowels[0].add(len(t))\n",
    "        word_vowels[1].add(sum([1 for i in t if i.lower() in 'aeiou']))\n",
    "\n",
    "    return word_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"supercalifragilisticexpialidocious\", \"eye\", \"Hawai'i\", \"draft\"]\n",
    "find_length_word_number_vowels(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using lists instead of sets will obviate this problem:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length_word_number_vowels_by_order_of_appearance(text):\n",
    "    \"\"\"\n",
    "    Returns an array of two lists.  The first set comprises\n",
    "    the lengths of the words.  The second the number of vowels.\n",
    "    For the sake of simplicity, 'y' is not counted as a vowel.\n",
    "    Results are ordered by order of appearance of the original\n",
    "    word.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vowels = [[], []]\n",
    "\n",
    "    for t in text:\n",
    "        word_vowels[0].append(len(t))\n",
    "        \n",
    "        # taking advantage of the fact that `True` evaluates to 1\n",
    "        word_vowels[1].append(sum([1 for i in t if i.lower() in 'aeiou']))\n",
    "\n",
    "    return word_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_length_word_number_vowels_by_order_of_appearance(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "◑ Write a function `novel10(text)` that prints any word that appeared in the last 10% of a text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novel10(text):\n",
    "    \"\"\"\n",
    "    Returns a set of words that appear for the first time\n",
    "    in the last 10% of a text.\n",
    "    \"\"\"\n",
    "\n",
    "    split = int(len(text) * .9)\n",
    "    first90, last10 = text[:split], text[split:]\n",
    "    novel90 = set(first90)\n",
    "\n",
    "    return set([i for i in last10 if i not in novel90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Mary Shelley's Frankenstein\n",
    "url = 'http://www.gutenberg.org/cache/epub/42324/pg42324.txt'\n",
    "\n",
    "frank = request.urlopen(url).read().decode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank = word_tokenize(frank)\n",
    "\n",
    "# used trial and error (and `index`) to find and remove\n",
    "# header and footer\n",
    "frank = frank[115:90732]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converted results to a list so that I could use slicing to get the first 20 items:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(novel10(frank))[:20], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15.\n",
    "\n",
    "◑ Write a program that takes a sentence expressed as a single string, splits it and counts up the words. Get it to print out each word and the word's frequency, one per line, in alphabetical order.\n",
    "\n",
    "*Although it's not best practice stylistically, the instructions say to print everything, so I'm going to do that instead of returning a value:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def print_words_and_frequency(text):\n",
    "    \"\"\"\n",
    "    Counts the words in a text and prints out the\n",
    "    resulting table in alphabetical order.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizer separates words from punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    words = [t.lower() for t in tokens if t.isalpha()]\n",
    "    \n",
    "    # get word counts from FreqDist\n",
    "    ordered = sorted(set([(w, v) for w, v in nltk.FreqDist(words).items()]))\n",
    "    \n",
    "    # get widths for pretty printing\n",
    "    # width of longest word\n",
    "    width = max([len(w) for w, _ in ordered]) + 2\n",
    "    # width of longest number\n",
    "    width_counts = max([len(str(v)) for _, v in ordered])\n",
    "    \n",
    "    # print everything\n",
    "    for w, v in ordered:\n",
    "        print(\"{}:{}{:>{}}\".format(w, ' ' * (width - len(w)), \n",
    "                                     v, width_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"If police police police police, who polices the police police? \" \\\n",
    "       \"Police police police police police police.\"\n",
    "\n",
    "print_words_and_frequency(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "print_words_and_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16.\n",
    "\n",
    "◑ Read up on Gematria, a method for assigning numbers to words, and for mapping between words having the same number to discover the hidden meaning of texts (cf. [here](http://en.wikipedia.org/wiki/Gematria \"Wikipedia entry\"), or [here](http://essenes.net/gemcal.htm \"Gematria\")).\n",
    "\n",
    "* a. Write a function `gematria()` that sums the numerical values of the letters of a word, according to the letter values in `letter_vals`:\n",
    "\n",
    "``` \t\n",
    ">>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
    "... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
    "... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
    "```\n",
    "\n",
    "* b. Process a corpus (e.g. `nltk.corpus.state_union`) and for each document, count how many of its words have the number 666.\n",
    "\n",
    "* c. Write a function `decode()` to process a text, randomly replacing words with their Gematria equivalents, in order to discover the \"hidden meaning\" of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
    "       'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
    "       'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
    "\n",
    "def gematria(word, vals = letter_vals):\n",
    "    \"\"\"\n",
    "    Returns Gematria value of a word.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    word: Word to be converted.\n",
    "    vals: Dictionary of values for each letter.\n",
    "          Default is `letter_vals`.\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(vals[w.lower()] for w in word if w in vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = nltk.corpus.state_union\n",
    "\n",
    "# for pretty printing later\n",
    "width = max([len(u[5:-4]) for u in su.fileids()])\n",
    "\n",
    "devil_words = [sum([1 for w in su.words(u) if gematria(w) == 666]) for u in su.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, w in zip(su.fileids(), devil_words):\n",
    "    print(u[:4], u[5:-4] + \":\" +\n",
    "          \" \" * (width - len(u[5:-4])),  \"{:>2}\".format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__ Since strings are immutable, we need to convert whatever format the `text` is in to a list. From there, we have several ways to choose words at random.  I originally chose $n$% of the index values at random and replaced those words.  But this was very slow with large texts, as the interpreter would have to traverse the entire list of words to find the word to be replaced.  I settled on the method below, which is much quicker: The interpreter only goes through the list of words once, and returns a random number between 0.0 and 1.0 for each word.  If the random number is below a certain threshold, the word is replaced.  With this method, there is a decent chance that the percentage of words replaced will differ from the percentage we specify, since the chances of one word being replaced are independent of all other words (i.e., as long as the threshold is not 0% or 100%, there's a chance that none of the words will be replaced, and also a chance that all of the words will be replaced).  Also, punctuation will never be replaced, since these marks don't have values in the dictionary `letter_vals`, which we use in the `gematria` function.*\n",
    "\n",
    "*Also using a function I used in [Chapter 2](https://github.com/Sturzgefahr/Natural-Language-Processing-with-Python-Analyzing-Text-with-the-Natural-Language-Toolkit/tree/master/Chapter%2002 \"Chapter 2 Exercises\") - `join_punctuation` - to reattach punctuation that was separated during tokenizing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current\n",
    "\n",
    "def decode(text, n = 10):\n",
    "    \"\"\"\n",
    "    Returns a copy of the original text with n percent\n",
    "    of the words converted into its gematria form.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: Text can be any form. Will be converted into a list.\n",
    "    n:    Percentage of the words to be converted.  Should be\n",
    "          an integer between 0 and 100.   \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # convert a cp of text into a list\n",
    "    if type(text) == str:\n",
    "        # use tokenize to separate punctuation from words\n",
    "        cp = word_tokenize(text)    \n",
    "    elif type(text) == list:\n",
    "        cp = text[:]    \n",
    "    else:\n",
    "        cp = list(text[:])\n",
    "        \n",
    "\n",
    "    # go through the words in the list,\n",
    "    # and return a random number;\n",
    "    # if the random number is less than the percentage\n",
    "    # replace the word with its gematria value\n",
    "    for i in range(len(cp)):\n",
    "        if random.random() <= n/100: \n",
    "            cp[i] = str(gematria(cp[i]))\n",
    "        \n",
    "    \n",
    "    # using join punctuation to rejoin punctuation separated during\n",
    "    # tokenizing\n",
    "    return ' '.join(join_punctuation(cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(\"This is just a test to see if my code will work\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(su.words('2006-GWBush.txt'), 33)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(nltk.corpus.gutenberg.words('austen-emma.txt'), 30)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "◑ Write a function `shorten(text, n)` to process a text, omitting the $n$ most frequently occurring words of the text. How readable is it?\n",
    "\n",
    "*The texts are usually quite readable if we delete the most common words, as most of these common words will be stop words.  However, in news articles and novels some of the most common words will be the names of the principals, and without those it's impossible to know who's doing what.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "\n",
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current\n",
    "\n",
    "\n",
    "def shorten(text, n):\n",
    "       \n",
    "    # convert a cp of text into a list\n",
    "    if type(text) == str:\n",
    "        # use tokenize to separate punctuation from words\n",
    "        cp = word_tokenize(text)    \n",
    "    elif type(text) == list:\n",
    "        cp = text[:]    \n",
    "    else:\n",
    "        cp = list(text[:])\n",
    "        \n",
    "    # get a list of most common words, and strip away the counts\n",
    "    most_common = [w for w, _ in nltk.FreqDist(w for w in cp if w.isalpha()).most_common(n)]\n",
    "    \n",
    "    # replace most common words\n",
    "    for i in range(len(cp)):\n",
    "        if cp[i] in most_common:\n",
    "            cp[i] = ''\n",
    "    \n",
    "    # join list and normalize whitespace - \n",
    "    # otherwise there'll be gaps for the missing words\n",
    "    # also use join_punctuation to reattach punctuation \n",
    "    # separate during tokenizing\n",
    "    return re.sub(r'\\s+', ' ', ' '.join(join_punctuation(cp)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(\"This is a test which is a rather short one\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(su.words('2006-GWBush.txt'), 50)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(nltk.corpus.gutenberg.words('austen-emma.txt'), 50)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we delete an additional 50 common words, we'll lose mentions of \"Woodhouse\", one of the principal characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(nltk.corpus.gutenberg.words('austen-emma.txt'), 100)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18.\n",
    "\n",
    "◑ Write code to print out an index for a lexicon, allowing someone to look up words according to their meanings (or pronunciations; whatever properties are contained in lexical entries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'm a little confused by this question, since the notion of lexicon indexing was not introduced in the book, nor is it such a common concept (i.e., googling it doesn't bring up that many hits).*\n",
    " \n",
    "*From the best that I can understand, the idea is that we make a list of all items in lexicon $X$ with quality $Y_1$, all items with $Y_2$, and so on until we get to all items with $Y_n$.*\n",
    "\n",
    "*If we used the CMU Pronouncing Dictionary for this, the first index would be for words with the phoneme `AA0`, the next for words with `AA1`, and so on until we get to the last phoneme, `Y` (cf. [here](http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=C+M+U+Dictionary \"CMU dictionary\") for more information).*\n",
    "\n",
    "*It's difficult to see how we could do this with anything other than a Python dictionary, which really haven't been covered in much detail.  A Python dictionary can create an index of all the phonemes of the 133,737 words in a matter of seconds.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries in the dictionary\n",
    "\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The dictionary uses the method `setdefault`, whose syntax I find quite twisted.  Regardless, the method creates a new item for phonemes it hasn't yet seen, and adds values for items that already exist.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for word, pronunciation in entries:\n",
    "        for phoneme in pronunciation:\n",
    "            # create item if it doesn't exist\n",
    "            # if it does exist, add current word\n",
    "            d.setdefault(phoneme, []).append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are 70 phonemes in this dictionary.  There are 39 basic phonemes, but the vowels have multiple forms, depending where the stress lies.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for x in d.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Now I'll create a function that will find all items in the lexicon that have some specified properties.  We need at least one property to match (that of `arg1`), but we can use an unlimited number of additional properties, thanks to the `*args` argument.  From there, we'll use set intersections to find only those items which have all the specified properties.</i>\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entries(d, arg1, *args):\n",
    "        \"\"\"\n",
    "        Returns a set of words using the phonemes\n",
    "        in arg1 and args.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        d:    A dictionary of items.\n",
    "        arg1: The first index to be match.\n",
    "        args: Optional 2nd - nth indices to be matched.\n",
    "        \"\"\"\n",
    "        \n",
    "        rest = set(d[arg1])\n",
    "        for a in args:\n",
    "            rest = rest.intersection(set(d[a]))\n",
    "        \n",
    "        return rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d, 'AE2', 'B', 'D', 'IH0', 'K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'd like to do a sanity check to make sure my code is working correctly.  Finding pronunciations for specific words in the CMU dictionary is not so straight-forward, so I've made a function for this.  I'll check just two words, since I still have to verify everything manually:*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronuncation(corpus, target):\n",
    "        \"\"\"\n",
    "        Return pronunciation for target in corpus.\n",
    "        \"\"\"\n",
    "            \n",
    "        return [pronunciation for word, pronunciation in corpus if word == target]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pronuncation(entries, 'abstracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pronuncation(entries, 'bundesbank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It takes a little longer to make a comparable dictionary of WordNet entries, but it's still possible.*\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}\n",
    "    \n",
    "for ss in wn.all_synsets():\n",
    "    for word in ss.definition().split():\n",
    "        word = re.sub(r\"[`'();,]\", \"\", word)\n",
    "        d2.setdefault(word, []).append(ss.name())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"able\", \"person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"able\", \"person\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"sound\", \"instrument\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"sound\", \"instrument\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"electricity\", \"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"electricity\", \"device\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.\n",
    "\n",
    "◑ Write a list comprehension that sorts a list of WordNet synsets for proximity to a given synset. For example, given the synsets `minke_whale.n.01`, `orca.n.01`, `novel.n.01`, and `tortoise.n.01`, sort them according to their `shortest_path_distance()` from `right_whale.n.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "\n",
    "whales = [orca, minke, tortoise, novel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set([(right.path_similarity(w), w) for w in whales]), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note the alternate names for __minke__ and __orca__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minke, orca, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. \n",
    "\n",
    "◑ Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word `table` and 9 instances of the word `chair`, then `table` would appear before `chair` in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_by_frequency(t):\n",
    "    return sorted(set([(w) for w, _ in nltk.FreqDist(t).items()]), \n",
    "                  reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(9):\n",
    "    test.append(\"chair\")\n",
    "for i in range(10):\n",
    "    test.append(\"table\")\n",
    "\n",
    "print(test, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_list_by_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.\n",
    "\n",
    "◑ Write a function that takes a text and a vocabulary as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this in a single line, using `set.difference()`?\n",
    "\n",
    "*The instructions explicitly say that we only need a single line and the arguments are lists of strings.  However, considering all the advice given in this chapter, I would strongly recommend programming \"defensively\" and adding checks to make sure that the arguments are indeed lists.*\n",
    "\n",
    "*Here's what the function would look like as one line:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_vocab_not_in_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Returns strings in text but not in vocab.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text:  A list of strings.\n",
    "    vocab: A list of strings.\n",
    "    \"\"\"\n",
    "    return set(text).difference(set(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we program defensively and try to envision every combination of `str`, `set`, or `list`, we'd end up with a much longer function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def return_vocab_not_in_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Returns strings in text but not in vocab.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text:  A list of strings, a set of strings, or a string.\n",
    "    vocab: A list of strings, a set of strings, or a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if text and vocab are some combination of set and list\n",
    "    if isinstance(text, set) and isinstance(vocab, set):\n",
    "        return text.difference(vocab)\n",
    "    elif isinstance(text, list) and isinstance(vocab, set):\n",
    "        return set(text).difference(vocab)\n",
    "    elif isinstance(text, set) and isinstance(vocab, list):\n",
    "        return text.difference(set(vocab))\n",
    "    \n",
    "    # if text is a str\n",
    "    if type(text) == str:\n",
    "        text = word_tokenize(text)\n",
    "    else:\n",
    "        assert isinstance(text, \n",
    "                          list), \"Argument `text` must be a list or a string\"\n",
    "    \n",
    "    # if vocab is a str\n",
    "    if type(vocab) == str:\n",
    "        vocab = word_tokenize(vocab)\n",
    "    else:\n",
    "        assert isinstance(vocab, \n",
    "                          list), \"Argument `vocab` must be a list or a string\"\n",
    "\n",
    "    return set(text).difference(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deep_thought = (\"When I was a kid my favorite relative was Uncle Caveman. \" \n",
    "                \"After school we'd all go play in his cave, \"\n",
    "                \"and every once in a while he would eat one of us. \" \n",
    "                \"It wasn't until later that I found out that Uncle Caveman \"\n",
    "                \"was a bear.\")\n",
    "\n",
    "# convert deep_thought to a list\n",
    "\n",
    "dt_words = word_tokenize(deep_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =  [\"'d\", 'was', '.', 'play', 'school', 'favorite', 'found',  \n",
    "          'kid', 'all', 'once', 'Caveman', 'Uncle', 'cave', \n",
    "          'while', 'relative', \"n't\", 'until', 'out', 'we', 'a', 'my', \n",
    "          'After', 'that', 'every', 'later', 'and', 'go', 'in', 'of', \n",
    "          'one', 'bear', 'When', 'would', 'eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(dt_words, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(deep_thought, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(set(dt_words), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.\n",
    "\n",
    "◑ Import the `itemgetter()` function from the operator module in Python's standard library (i.e. `from operator import itemgetter`). Create a list `words` containing several words. Now try calling: `sorted(words, key=itemgetter(1))`, and `sorted(words, key=itemgetter(-1))`. Explain what `itemgetter()` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "words = ['my', 'list', 'of', 'several', 'words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`itemgetter(n)` retrieves the item at index position `n`. In the below examples, the list `word` is sorted by the key at index `n`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If the above examples were not clear, the tuple below should be easier to follow:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [('A', 'Beta', 3), ('C', 'Alpha', 2), ('B', 'Gamma', 1)]\n",
    "sorted(test, key = itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test, key = itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test, key = itemgetter(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. \n",
    "\n",
    "◑ Write a recursive function `lookup(trie, key)` that looks up a key in a trie, and returns the value it finds. Extend the function to return a word when it is uniquely determined by its prefix (e.g. `vanguard` is the only word that starts with `vang-`, so `lookup(trie, 'vang')` should return the same thing as `lookup(trie, 'vanguard'))`.\n",
    "\n",
    "*I tried to use recursion as much as possible when solving this question.  I was able to do this for the first part of the question - simple lookup of a word in the tree; but I had some problems when I tried to expand this so that we could find words solely by their prefix.  If a word was uniquely determined by its prefix - as per the example in the question - then I was able to return the value solely through recursion. However, for non-unique prefixes, there would be several potential matches, but this function would only return the first, without notifying the user that there were other possible matches. I found this of limited practical usage, since an end user would probably not be aware if a prefix was unique or not. I therefore expanded the function even more so that it output all possible final values.  To tackle this final problem I had to resort to a non-recursive method - namely, a second function that would handle the special case of non-unique prefixes.  As far as I am aware, it would not be possible to do this entirely recursively.*\n",
    "\n",
    "*The function `insert` from chapter 4 in the NLTK book:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(trie, key, value):\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in trie:\n",
    "            trie[first] = {}\n",
    "        insert(trie[first], rest, value)\n",
    "    else:\n",
    "        trie['value'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = {}\n",
    "\n",
    "en = [\"vandalism\", \"vandalize\", \"vane\", \"vanguard\", \"vanilla\", \"vanish\", \n",
    "      \"vanity\"]\n",
    "fr = [\"vandalisme\", \"vandaliser\", \"girouette\", \"avant-garde\", \"vanille\", \n",
    "      \"disparaître\", \"vanité\"]\n",
    "\n",
    "[insert(trie, e, f) for e, f in zip(en, fr)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': {'h': {'a': {'i': {'r': {'value': 'flesh'}},\n",
      "                   't': {'value': 'cat'}},\n",
      "             'i': {'c': {'value': 'stylish'},\n",
      "                   'e': {'n': {'value': 'dog'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(trie, width = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The original version of the function that I wrote.  It looks up a key and returns its value:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        return trie[word]     \n",
    "    first, rest = word[0], word[1:]\n",
    "    if first not in trie:\n",
    "        return False\n",
    "    return lookup(trie[first], rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - word not in trie\n",
    "\n",
    "lookup(trie, 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The expanded version is able to handle portions of keys, as long as those portions are unique to a specific key.  Otherwise, the function merely returns the first value to match that portion of a key.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded version - returns the value for complete words and unique suffixes\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    Word can be a complete word of just a prefix, i.e., \n",
    "    beginning of the string.  Function will only return the \n",
    "    first match for non-unique prefixes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)\n",
    "\n",
    "    first, rest = word[0], word[1:]\n",
    "    if first not in trie:\n",
    "        return False\n",
    "    return lookup(trie[first], rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vanil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The final version is able to handle non-unique portions of keys, but resorts to a helper function to do this.  Therefore, it is not entirely recursive:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi-recursive version; able to handle non-unique prefixes\n",
    "\n",
    "def get_final_value(trie, word):\n",
    "    \"\"\"\n",
    "    Retrieves the final value of word\n",
    "    in a trie.\n",
    "    \"\"\"\n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "\n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)     \n",
    "    return results\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    Word can be a complete word of just a prefix, i.e., \n",
    "    beginning of the string.  Function will return all \n",
    "    possible matches for non-unique prefixes.\n",
    "    \"\"\"\n",
    "        \n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "        if len(keys) > 1:\n",
    "            results = []\n",
    "            for k in keys:\n",
    "                results.append(get_final_value(trie[word], k))\n",
    "            return results\n",
    "        \n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)\n",
    "            \n",
    "        \n",
    "    if len(word) > 1:\n",
    "        \n",
    "        first, rest = word[0], word[1:]\n",
    "        if first not in trie:\n",
    "            return False\n",
    "        return lookup(trie[first], rest)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.\n",
    "\n",
    "◑ Read up on \"keyword linkage\" (chapter 5 of (Scott & Tribble, 2006)). Extract keywords from NLTK's Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks.\n",
    "\n",
    "*This was another question that I found to be quite vexing. Outside of this question, there is no mention of 'keyword linkage' in the book.  Furthermore, the Shakespeare Corpus is in xml format, which isn't covered until the last chapter of the book.*\n",
    "\n",
    "*There is some example code about the Shakespeare Corpus [here](http://www.nltk.org/howto/corpus.html, \"howto/corpus\"), but in my opinion it's not terribly helpful.  I was only able to access the lines of the plays after I had located an [external blog](https://www.datasciencebytes.com/bytes/2014/12/30/topic-modeling-of-shakespeare-characters/ \"topic modeling in Shakespeare\") where someone discussed dealing with a similar problem.*\n",
    "\n",
    "*Furthermore, even after I was able to locate a copy of Scott & Tribble, I found it quite difficult to replicate their results.  I found their discussion of keywords to be very general, and it was difficult to get the exact keywords in \"Romeo & Juliet\" that they had found.*\n",
    "\n",
    "*Finally, I'm not convinced that NetworkX is the best module for this specific task.  The example in the book showed WordNet hierarchies, but the network in this example is much shallower.  The networks for this problem have a lot of nodes and only a few edges, which make for a very cluttered graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The xml text is stored in this format, making it difficult to grab text from collocational windows (i.e., 5 words before and after a keyword).*\n",
    "\n",
    "\n",
    "```{xml}\n",
    "<SPEECH>\n",
    "<SPEAKER>BENVOLIO</SPEAKER>\n",
    "<LINE>Why, Romeo, art thou mad?</LINE>\n",
    "</SPEECH>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "from xml.etree import ElementTree\n",
    "shakespeare.fileids() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "personae = [persona.text for persona in\n",
    "             play.findall('PERSONAE/PERSONA')]\n",
    "print(personae) # doctest: +ELLIPSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = set(speaker.text for speaker in\n",
    "                play.findall('*/*/*/SPEAKER'))\n",
    "\n",
    "print(speakers, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Not quite what I had wanted, but a dictionary of all lines with that mention Romeo:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Romeo = {}\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "for act in play.findall('ACT'):\n",
    "    for scene in act.findall('SCENE'):\n",
    "        for speech in scene.findall('SPEECH'):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'Romeo' in str(line.text):\n",
    "                    print(line.text)\n",
    "                    for word in word_tokenize(line.text):\n",
    "                        Romeo[word] = 1 + Romeo.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Romeo, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>[The blog](https://www.datasciencebytes.com/bytes/2014/12/30/topic-modeling-of-shakespeare-characters/ \"Topic modeling in Shakespeare\") that I referenced earlier had a method for making a dictionary where all of the characters in a play would be the keys, and all of their lines the values.  What follows below is modified code from what was found at the linked blog:<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "lines = defaultdict(list)\n",
    "linecounts = defaultdict(int)\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "for child in play.findall('ACT/SCENE/SPEECH'):\n",
    "    speaker = child.find('SPEAKER').text\n",
    "    for line in child.findall('LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    lines[speaker].append(word)\n",
    "                    linecounts[speaker] += 1\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a dictionary of all the words said by Romeo, followed by their counts:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romeo = {}\n",
    "\n",
    "for w in lines[\"ROMEO\"]:\n",
    "    romeo[w] = 1 + romeo.get(w, 0)\n",
    "    \n",
    "from operator import itemgetter\n",
    "\n",
    "print(sorted(romeo.items(), key = itemgetter(1), reverse = True)[:20], end = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting raw text from Romeo & Juliet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "raw_rj = []\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "    if line.text is not None:\n",
    "        for word in tokenizer.tokenize(line.text):\n",
    "            if word not in stopwords and len(word) > 2:\n",
    "                raw_rj.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_rj[:20], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting raw text from other Shakespeare plays in this corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_others = []\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "for p in shakespeare.fileids():\n",
    "    # tokenize all plays EXCEPT R & J\n",
    "    if p != 'r_and_j.xml':\n",
    "        play = shakespeare.xml(p)\n",
    "    \n",
    "    \n",
    "    for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    raw_others.append(word)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One way to think of keywords is all the words in one text that do not appear in comparable texts.  Here, `rj_kw` is the set difference between all the words in \"Romeo and Juliet\" and all the words in the other plays in the corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_kw = set(raw_rj) - set(raw_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Romeo\" in rj_kw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But we should get rid of __hapax legomenon__ (words that only occur once), as well as __dis legomenon__ (words that occur only twice):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts in raw R&J text\n",
    "\n",
    "raw_rj_dict = {}\n",
    "\n",
    "for w in raw_rj:\n",
    "    raw_rj_dict[w] = 1 + raw_rj_dict.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary\n",
    "\n",
    "potential_rj_keywords = sorted(raw_rj_dict.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the words that appear more than twice\n",
    "\n",
    "rj_keywords = [w for w, v in potential_rj_keywords if v > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rj_keywords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below makes a master list of keywords in the plays other than R & J in our corpus.  In order to make it to the master list, a word has to appear at least twice in at least one play.  This code is basically a combination of other pieces of code that have been used up to now:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "# master list of keywords in all plays\n",
    "raw_others_keywords = []\n",
    "\n",
    "for p in shakespeare.fileids():\n",
    "    # tokenize all plays EXCEPT R & J\n",
    "    if p != 'r_and_j.xml':\n",
    "        play = shakespeare.xml(p)\n",
    "    \n",
    "    # temporary file for all words in current play\n",
    "    temp_raw = []\n",
    "    \n",
    "    # append all words to temp file\n",
    "    for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    temp_raw.append(word)\n",
    "                    \n",
    "    # make temporary dictionary and tally words in current play\n",
    "    temp_d = {}\n",
    "    for w in temp_raw:\n",
    "        temp_d[w] = 1 + temp_d.get(w, 0)\n",
    "\n",
    "    # if word occurs more than twice, add to master list\n",
    "    for w, v in temp_d.items():\n",
    "        if v > 2:\n",
    "            raw_others_keywords.append(w)\n",
    "            \n",
    "# get rid of duplicates\n",
    "raw_others_keywords = set(raw_others_keywords)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substract kws from all plays from kws in R & J\n",
    "rj_only_kw = set(rj_keywords) - raw_others_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by count\n",
    "\n",
    "potential_rj_keywords = sorted(raw_rj_dict.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those words that appear more than twice, and only in R & J\n",
    "\n",
    "rj_kw = sorted(set(w for w,v in potential_rj_keywords if w in rj_only_kw and v >= 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rj_kw, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is very close to the method described in Scott and Tribble, but my list of resulting keywords seems to be much larger.  It should be remember that Scott & Tribble used a much larger corpus of Shakespeare works.*\n",
    "\n",
    "*Next, I tried to make a dictionary for each of the main characters, with the most common words used by each:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romeo = {}\n",
    "\n",
    "for w in lines[\"ROMEO\"]:\n",
    "    if w in rj_kw:\n",
    "        romeo[w] = 1 + romeo.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(romeo.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tybalt = {}\n",
    "\n",
    "for w in lines[\"TYBALT\"]:\n",
    "    if w in rj_kw:\n",
    "        tybalt[w] = 1 + tybalt.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tybalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juliet = {}\n",
    "\n",
    "for w in lines[\"JULIET\"]:\n",
    "    if w in rj_kw:\n",
    "        juliet[w] = 1 + juliet.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(juliet.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse = {}\n",
    "\n",
    "for w in lines[\"NURSE\"]:\n",
    "    if w in rj_kw:\n",
    "        nurse[w] = 1 + nurse.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mercutio = {}\n",
    "\n",
    "for w in lines[\"MERCUTIO\"]:\n",
    "    if w in rj_kw:\n",
    "        mercutio[w] = 1 + mercutio.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mercutio.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris = {}\n",
    "\n",
    "for w in lines[\"PARIS\"]:\n",
    "    if w in rj_kw:\n",
    "        paris[w] = 1 + paris.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I then took five of the main characters, and created dictionaries with the ten keywords most often used by each charcter.  I used these dictionaries to produce a network graph:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom = [w for w, _ in sorted(romeo.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "jul = [w for w, _ in sorted(juliet.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "tyb = [w for w, _ in sorted(tybalt.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "mer = [w for w, _ in sorted(mercutio.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "par = [w for w, _ in sorted(paris.items(), key = itemgetter(1), reverse = True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Romeo': rom, 'Juliet': jul, 'Tybalt': tyb, 'Mercutio': mer, 'Paris': par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as  nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = nx.DiGraph(d)\n",
    "\n",
    "g.add_nodes_from(d.keys())\n",
    "\n",
    "nx.draw(g, with_labels=True, node_size = 100,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring layout\n",
    "g = nx.DiGraph(d)\n",
    "\n",
    "g.add_nodes_from(d.keys())\n",
    "\n",
    "pos = nx.spring_layout(g)\n",
    "\n",
    "nx.draw_networkx(g, pos, node_size = 100, edge_color='y', alpha=.8, linewidths=0, font_size = 12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The resulting graph isn't very helpful.  Let's use the exact keywords given in Scott & Tribble.  To get these keywords, the authors made a list of keywords occurring within a collocation window of five words before or after the target words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMEO = [\"Benvolio\", \"Juliet\", \"banished\", \"night\", \"Tybalt\", \"art\", \"dead\", \n",
    "         \"O\", \"thou\", \"is\"]\n",
    "TYBALT = [\"Capulet\", \"dead\", \"kinsman\", \"art\", \"O\", \"Mercutio\", \"slain\", \"is\", \n",
    "          \"thou\", \"Romeo\"]\n",
    "JULIET =  [\"she\", \"O\", \"lady\", \"thou\", \"thee\", \"thy\", \"dead\", \"Romeo\", \"Nurse\"]\n",
    "CAPULET = [\"thee\", \"Friar\", \"Tybalt\", \"Juliet\", \"Montague\", \"is\", \"Paris\", \n",
    "           \"Capulet’s\", \"nurse\", \"Lady\"]\n",
    "NURSE = [\"thy\", \"she\", \"Peter\", \"thee\", \"is\", \"Juliet\", \"Capulet\", \"thou\", \n",
    "         \"Lady\", \"O\"]\n",
    "NIGHT = [\"light\", \"torch\", \"O\", \"she\", \"thy\", \"thee\", \"love\", \"Romeo\", \"thou\", \"is\"]\n",
    "MERCUTIO = [\"she\", \"O\", \"kinsman\", \"is\", \"thy\", \"thou\", \"lady\", \"Romeo\", \n",
    "            \"Tybalt\", \"Benvolio\"]\n",
    "PARIS = [\"Thursday\", \"married\", \"Lawrence\", \"Friar\", \"love\", \"is\", \"Romeo\", \n",
    "         \"dead\", \"Capulet\", \"County\"]\n",
    "LOVE = [\"Paris\", \"Lady\", \"death\", \"night\", \"she\", \"thee\", \"thou\", \"is\", \"O\", \n",
    "        \"thy\"]\n",
    "MONTAGUE = [\"thee\", \"Benvolio\", \"O\", \"thy\", \"art\", \"Lady\", \"Romeo\", \"thou\", \n",
    "            \"Capulet\", \"is\"]\n",
    "THOU = [\"death\", \"night\", \"O\", \"love\", \"is\", \"Romeo\", \"thee\", \"wilt\", \"thy\", \n",
    "        \"art\"]\n",
    "FRIAR = [\"Romeo\", \"Nurse\", \"Capulet\", \"Lady\", \"Mantua\", \"Paris\", \"O\", \"is\", \n",
    "         \"cell\", \"Lawrence\"]\n",
    "ROMEOS = [\"she\", \"dead\", \"banished\", \"Romeo\", \"thou\", \"Friar\", \"Tybalt’s\", \n",
    "          \"watch\", \"O\", \"is\"]\n",
    "O = [\"Juliet\", \"Friar\", \"she\", \"Nurse\", \"thee\", \"thy\", \"Romeo\", \"thou\", \"love\", \n",
    "     \"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'Romeo': ROMEO, 'Tybalt': TYBALT, 'Juliet': JULIET, 'Capulet': CAPULET, 'n': NURSE, 'ni': NIGHT, \n",
    "      'm': MERCUTIO, 'p': PARIS, 'l': LOVE, 'mo': MONTAGUE, 'th': THOU, 'f': FRIAR, 'rs': ROMEOS, 'o': O}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = nx.DiGraph(d2)\n",
    "\n",
    "g2.add_nodes_from(d2.keys())\n",
    "\n",
    "\n",
    "for k, v in d2.items():\n",
    "    g2.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g2, with_labels=True, node_size = 100,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = nx.DiGraph(d2)\n",
    "\n",
    "g2.add_nodes_from(d2.keys())\n",
    "\n",
    "\n",
    "for k, v in d2.items():\n",
    "    g2.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g2, with_labels=True)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This graph is much too busy.  Let's try a smaller network, with just the two principals and the main characters from the house of Montague:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = {'Romeo': ROMEO,  'Juliet': JULIET,  'Mercutio': MERCUTIO,  'Montague': MONTAGUE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = nx.DiGraph(d3)\n",
    "\n",
    "g3.add_nodes_from(d3.keys())\n",
    "\n",
    "\n",
    "for k, v in d3.items():\n",
    "    g3.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g3, with_labels=True, node_size = 10,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try the same with the house of Capulet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = {'Romeo': ROMEO,  'Juliet': JULIET,  'Capulet': CAPULET, 'Nurse': NURSE, 'Paris': PARIS,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4 = nx.DiGraph(d4)\n",
    "\n",
    "g4.add_nodes_from(d4.keys())\n",
    "\n",
    "\n",
    "for k, v in d4.items():\n",
    "    g4.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g4, with_labels=True, node_size = 10,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below was code I used to try to replicate the keyword selection in Scott & Tribble.  The code below find words within a collocation window of five words before and after a mention of 'Romeo'. The XML file lists each line separately, and if a target word began a line, it would be difficult to grab the last five words from the previous line.  To simplify things, I stripped all the lines of text from the XML file and concatenated it to one long list.  To prevent the words of two speakers from appearing in two collocation windows, I added five empty strings before the beginning of every speaker's line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'?\\w+\")\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "raw = []\n",
    "\n",
    "for child in play.findall('ACT/SCENE/SPEECH'):\n",
    "    speaker = child.find('SPEAKER').text\n",
    "    for i in range(5):\n",
    "        raw.append(\"\")\n",
    "    for line in child.findall('LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                raw.append(word)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw[:150], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words from collocation windows with 'Romeo'\n",
    "romeo_ky = [raw[i - 5 :i] + raw[i + 1 : i + 6] for i in range(len(raw)) if raw[i] == \"Romeo\"]\n",
    "\n",
    "# only keep those words in R&J keywords\n",
    "romeo_ky = [w for sl in romeo_ky for w in sl if w in rj_kw]\n",
    "\n",
    "# get counts\n",
    "romeo_kyd = {}\n",
    "for w in romeo_ky:\n",
    "    romeo_kyd[w] = 1 + romeo_kyd.get(w, 0)\n",
    "    \n",
    "# sort by count\n",
    "from operator import itemgetter\n",
    "   \n",
    "romeo_ky = [w for w, _ in sorted(romeo_kyd.items(), \n",
    "                                 key = itemgetter(1), reverse = True)]\n",
    "print(romeo_ky, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare that to the keywords listed in Scott & Tribble:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMEO = [\"Benvolio\", \"Juliet\", \"banished\", \"night\", \"Tybalt\", \"art\", \"dead\", \n",
    "         \"O\", \"thou\", \"is\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As I mentioned earlier, instead of giving a step-by-step breakdown of their process for determining keywords, Scott & Tribble instead presented it more as a narrative, making it quite difficult to replicate their results.*\n",
    "\n",
    "*When examining our - fairly disappointing - final graphs, something we have to remember is that Scott & Tribble never discussed actual linked keyword networks; rather hypothetical ones:*\n",
    "\n",
    "* \"We may now hypothesise that a KW plot such as the ones we saw in the previous chapter could be redrawn as a network of connections consisting of configurations reminiscent of strings, stars, cliques and clumps of which the next figure shows a fragment.\" (Scott & Tribble, p. 75)\n",
    "\n",
    "*From the graphs above, we can see that the actual graphs would be far messier than hypothesized.  The number of overlapping edges is large and makes the graph very difficult to read.  The only way to remedy this is to use a select subset of keywords for a selection of characters.  But the process of selecting keywords is quite laborious and might outweigh any benefits of seeing these networks visually.*\n",
    "\n",
    "*I would argue that this approach is not as practical as Scott & Tribble hypothesized.  While one may say that my personal inexperience with NetworkX is responsible for the less-than-ideal graphs above, a Google image search for \"linguistics keyword linkage graph\" failed to produce any relevant results. This seems to indicate that in the 13 years since \"Textual Patterns\" was published, no one has been able to actualize and publish any linked network graphs in the vein of what Scott & Tribble proposed, and this approach is not a viable one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. \n",
    "\n",
    "◑ Read about string edit distance and the Levenshtein Algorithm. Try the implementation provided in `nltk.edit_distance()`. In what way is this using dynamic programming? Does it use the bottom-up or top-down approach? [See also http://norvig.com/spell-correct.html]\n",
    "\n",
    "*The implementation uses a bottom-up approach: It breaks the problem into sub-problems, and solves these one by one until the edit distance has been found.  Since many of the sub-problems will have overlapping solutions, this implementation saves time by using dynamic programming, as the function merely retrieves the solution found earlier, instead of calculating it anew.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.edit_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('water', 'wine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('something', 'seething')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('boy', 'girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. \n",
    "\n",
    "◑ The Catalan numbers arise in many applications of combinatorial mathematics, including the counting of parse trees. The series can be defined as follows: $C_0 = 1$, and $C_{n+1} = \\sum_{0..n} (C_iC_{n-i})$.\n",
    "\n",
    "* a. Write a recursive function to compute $n$th Catalan number $C_n$.\n",
    "\n",
    "* b. Now write another function that does this computation using dynamic programming.\n",
    "\n",
    "* c. Use the timeit module to compare the performance of these functions as n increases.\n",
    "\n",
    "*After a little research, I discovered that the only way to get this to work as a recursive program is to rewrite the formula as $C_n = \\sum_0^n (C_iC_{n-1-i})$:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Catalan(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Catalan_memo(n, lookup = {0: 1}):\n",
    "    if n not in lookup:\n",
    "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
    "        lookup[n] = result\n",
    "    return lookup[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan_memo(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import memoize\n",
    "@memoize\n",
    "\n",
    "def Catalan_memoize(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan_memoize(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use `%%time` to time operations in jupyter notebooks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan_memo(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan_memoize(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This [website](https://rosettacode.org/wiki/Catalan_numbers#Python \"printing the results for multiple functions at once\") had some nice code for running several functions at once and printing the results in a presentable format.  Below is the code I found at the site, with slight modifications:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(defs, results):\n",
    "    fmt = '%-16s ' * 3\n",
    "    print((fmt % tuple(c.__name__ for c in defs)).upper())\n",
    "    print(fmt % ((\"=\" * 16,) * 3))\n",
    "    for r in zip(*results):\n",
    "        print(fmt % r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = (Catalan, Catalan_memo, Catalan_memoize)\n",
    "results = [tuple(c(i) for i in range(15)) for c in defs]\n",
    "pr(defs, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. \n",
    "\n",
    "★ Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship identification.\n",
    "\n",
    "*Zhao & Zobel is easily found online.  [Here](https://www.researchgate.net/publication/221574042_Searching_With_Style_Authorship_Attribution_in_Classic_Literature \"Zhao & Zobel\") is the site where I found my copy.*\n",
    "\n",
    "*Their study would be very difficult to replicate, especially for someone only on chapter 4 of the NLTK.  The study dealt with a huge number of texts (634 Project Gutenberg texts and 250,000 newswire articles), and some of the methods used are either NLP methods that have not been covered yet in this book (e.g., part-of-speech tagging), or fairly complicated statistical techniques that are beyond the purview of the book, i.e., the Kullback-Leibler divergence.*\n",
    "\n",
    "*Since I didn't want to reinvent the wheel for such a complicated experiment, I did a little googling to see if someone else had undertaken a comparable experiment.  I found one [here](http://www.aicbt.com/authorship-attribution/ \"Authorship Attribution\") that looked promising.  The author in this case use machine learning techniques to examine one book with two authors, and tried to determine which author had written which chapter.*\n",
    "\n",
    "*I aimed to analyze the same authors as in Zhao & Zobel - namely Shakespeare (WS) and Christopher Marlowe (CM) - and used six plays from Marlowe and seven from Shakespeare.  Since Marlowe was a tragedian, I used only tragedies for Shakespeare's works.  The thirteen works were \"Antony and Cleopatra\" (WS); \"Coriolanus\" (WS); \"Dido\" (CM); \"Dr. Faustus\" (CM); \"Edward II\" (CM); \"Hamlet\" (WS); \"The Jew of Malta\" (CW); \"Julius Caesar\" (WS); \"King Lear\" (WS); \"MacBeth\" (WS); \"Othello\" (WS); \"Tamburlaine pt. I\" (CM); and \"Tamburlaine pt. II\" (CM).*\n",
    "\n",
    "*Most of the code which follows is based on the code at that blog, with slight modifications for this task. Namely, the original code considered 12 chapters of a single book, whereas I considered 13 plays.  Also, the original code used punctuation as a method for attributing authors, specifically semicolons and colons.  However, we don't have the original texts of the majority (if not the entirety) of plays from Elizabethan era playwrights, so any differences in punctuation may be due to the editors of the various editions of these plays.  I.e., the differences in the use of semicolons/colons are probably not a good way to identify 16th/17th-century authors.  Nonetheless, I was curious if the number of exclamation marks could indicate who wrote which play, so I left this code in.*\n",
    "\n",
    "*All the texts examined were downloaded from Project Gutenberg (PG).  In the case where there were several editions of the same text, I chose the one with the most downloads.  Although I have written code to programmatically remove header and footer information from PG texts, for this experiment I decided to inspect each text manually: I was looking for editorial annotations that might cause the classifier to favor one author over the other, and deleted non-textual markers such as brackets used to designate footnotes and underscores used to designate the names of speakers before their spoken lines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, glob, os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts_Anon\"\n",
    "files = sorted(glob.glob(os.path.join(path, \"Text*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_text = ' '.join(texts)\n",
    "\n",
    "# get rid of carriage returns and other encoding remnants\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_text = re.sub(k, v, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalFeatures():\n",
    "    num_texts = len(texts)\n",
    "    fvs_lexical = np.zeros((num_texts, 3), np.float64)\n",
    "    fvs_punct = np.zeros((num_texts, 3), np.float64)\n",
    "\n",
    "    for e, n_text in enumerate(texts):\n",
    "        tokens = nltk.word_tokenize(n_text.lower())\n",
    "        words = word_tokenizer.tokenize(n_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(n_text)\n",
    "\n",
    "        vocab = set(words)\n",
    "\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) \n",
    "                                       for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Exclamation marks per sentence\n",
    "        fvs_punct[e, 2] = tokens.count('!') / float(len(sentences))\n",
    "        \n",
    "    \n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    \n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(n = 15):\n",
    "    # get most common words in the whole book\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "    fdist = nltk.FreqDist(all_tokens)\n",
    "    vocab = [k for k,v in fdist.most_common(n)]\n",
    "\n",
    "    # use sklearn to create the bag of words feature vector for each text\n",
    "    vectorizer = CountVectorizer(vocabulary = vocab, tokenizer = nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(texts).toarray().astype(np.float64)\n",
    "\n",
    "    # normalize by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "    \n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures():\n",
    "    # get part of speech for each token in each chapter\n",
    "\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    text_pos = [token_to_pos(t) for t in texts]\n",
    "\n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[t.count(pos) for pos in pos_list]\n",
    "                           for t in text_pos]).astype(np.float64)\n",
    "\n",
    "    # normalize by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(t) for t in text_pos])]\n",
    "    \n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters = 2, init = 'k-means++', n_init = 10, verbose = 0)\n",
    "    km.fit(fvs)\n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = list(LexicalFeatures())\n",
    "feature_sets.append(BagOfWords(15))\n",
    "feature_sets.append(SyntacticFeatures())\n",
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Third attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately, the results are far from conclusive, and a bit difficult to decipher.  The four rows refer to __Lexical differences__; __Punctuation differences__; __Bag of Words__; and __Syntactic differences__.  Since there is no guide telling us which author is '0' and which is '1', we have to deduce which is which. (Usually, Marlowe is '0' and Shakespeare '1').  The results are usually middling, and making matters worse is the fact the model is rather inconsistent: the results tend to vary widely each time the experiment is run.  Part of this is probably due to the fact that the model uses K-means clustering, which to a large extent is based on random initial values.  However, since the results seem to be so different each time, it seems that the features are quite difficult to distinguish from each other, and that a better set of features needs to be used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I decided to change tack and tried to replicate some of the findings from Zhao and Zobel.  Although my sample was much smaller, the initial figures I produced were quite different.  I focused on function words (closed set words such as articles, modal verbs, prepositions, etc...) and part-of-speech tags. For this experiment, I concatenated all the Shakespeare tragedies into one long string, and did the same for the Marlowe works:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "sh_files = sorted(glob.glob(os.path.join(path, \"Sh*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in sh_files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_sh = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_sh = re.sub(k, v, all_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Marlowe data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ma_files = sorted(glob.glob(os.path.join(path, \"Ma*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ma_files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_ma = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_ma = re.sub(k, v, all_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*List of function words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://semanticsimilarity.files.wordpress.com/2013/08/jim-oshea-fwlist-277.pdf\n",
    "\n",
    "function_words = [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "                  \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \n",
    "                  \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \n",
    "                  \"amongst\", \"amoungst\", \"an\", \"and\", \"another\", \"any\", \n",
    "                  \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \n",
    "                  \"are\", \"around\", \"as\", \"at\", \"be\", \"became\", \"because\", \n",
    "                  \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\",\n",
    "                  \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \n",
    "                  \"by\", \"can\", \"cannot\", \"could\", \"dare\", \"despite\", \"did\", \n",
    "                  \"do\", \"does\", \"done\", \"down\", \"during\", \"each\", \"eg\", \n",
    "                  \"either\", \"else\", \"elsewhere\", \"enough\", \"etc\", \"even\", \n",
    "                  \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \n",
    "                  \"except\", \"few\", \"first\", \"for\", \"former\", \"formerly\", \n",
    "                  \"from\", \"further\", \"furthermore\", \"had\", \"has\", \"have\", \n",
    "                  \"he\", \"hence\", \"her\", \"here\", \"hereabouts\", \"hereafter\", \n",
    "                  \"hereby\", \"herein\", \"hereinafter\", \"heretofore\", \n",
    "                  \"hereunder\", \"hereupon\", \"herewith\", \"hers\", \"herself\", \n",
    "                  \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \n",
    "                  \"if\", \"in\", \"indeed\", \"inside\", \"instead\", \"into\", \"is\", \n",
    "                  \"it\", \"its\", \"itself\", \"last\", \"latter\", \"latterly\", \n",
    "                  \"least\", \"less\", \"lot\", \"lots\", \"many\", \"may\", \"me\", \n",
    "                  \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \n",
    "                  \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"namely\", \n",
    "                  \"near\", \"need\", \"neither\", \"never\", \"nevertheless\", \n",
    "                  \"next\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \n",
    "                  \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \n",
    "                  \"oftentimes\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \n",
    "                  \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \n",
    "                  \"ourselves\", \"out\", \"outside\", \"over\", \"per\", \"perhaps\", \n",
    "                  \"rather\", \"re\", \"same\", \"second\", \"several\", \"shall\", \n",
    "                  \"she\", \"should\", \"since\", \"so\", \"some\", \"somehow\", \n",
    "                  \"someone\", \"something\", \"sometime\", \"sometimes\", \n",
    "                  \"somewhat\", \"somewhere\", \"still\", \"such\", \"than\", \"that\", \n",
    "                  \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \n",
    "                  \"thence\", \"there\", \"thereabouts\", \"thereafter\", \"thereby\",\n",
    "                  \"therefore\", \"therein\", \"thereof\", \"thereon\", \"thereupon\", \n",
    "                  \"these\", \"they\", \"third\", \"this\", \"those\", \"though\", \n",
    "                  \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                  \"too\", \"top\", \"toward\", \"towards\", \"under\", \"until\", \"up\", \n",
    "                  \"upon\", \"us\", \"used\", \"very\", \"via\", \"was\", \"we\", \"well\", \n",
    "                  \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \n",
    "                  \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "                  \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "                  \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \n",
    "                  \"why\", \"whyever\", \"will\", \"with\", \"within\", \"without\", \n",
    "                  \"would\", \"yes\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "                  \"yourselves\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proportion of 'the', 'of', and 'a' amongst the function words used in Shakespeare:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_words = word_tokenizer.tokenize(all_sh.lower())\n",
    "sh_function = [w for w in sh_words if w in function_words]\n",
    "sh_dict = {}\n",
    "\n",
    "f_words = ['the', 'of', 'a']\n",
    "\n",
    "for f in f_words:\n",
    "    for w in sh_function:\n",
    "        if w == f:\n",
    "            sh_dict[f] = 1 + sh_dict.get(f, 0)\n",
    "            \n",
    "for f in f_words:\n",
    "    sh_dict[f] = (sh_dict[f]/len(sh_function)) * 100\n",
    "    \n",
    "sh_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is slightly different from the results found in Zhao & Zobel, which were respectively 7.6, 4.8, and 4.1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_words = word_tokenizer.tokenize(all_ma.lower())\n",
    "ma_function = [w for w in ma_words if w in function_words]\n",
    "ma_dict = {}\n",
    "\n",
    "f_words = ['the', 'of', 'a']\n",
    "\n",
    "for f in f_words:\n",
    "    for w in ma_function:\n",
    "        if w == f:\n",
    "            ma_dict[f] = 1 + ma_dict.get(f, 0)\n",
    "            \n",
    "for f in f_words:\n",
    "    ma_dict[f] = (ma_dict[f]/len(ma_function)) * 100\n",
    "    \n",
    "ma_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The respective results from Zhao & Zobel: 9.5, 6.2, 3.2.*\n",
    "\n",
    "*I then tried to establish what percentage of POS tags were coordinating conjunctions (CC), prepositions (IN), and adjectives (JJ).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow to run\n",
    "\n",
    "sh_pos = [p[1] for p in nltk.pos_tag(all_sh)]\n",
    "ma_pos = [p[1] for p in nltk.pos_tag(all_ma)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_d = {}\n",
    "\n",
    "tags = ['CC', 'IN', 'JJ']\n",
    "\n",
    "for t in tags:\n",
    "    for p in sh_pos:\n",
    "        if p == t:\n",
    "            sh_pos_d[t] = 1 + sh_pos_d.get(t, 0)\n",
    "            \n",
    "for t in tags:\n",
    "    sh_pos_d[t] = (sh_pos_d[t]/len(sh_pos)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The results were quite different in Zhao & Zobel.  Respectively, they were 3.8, 5.9, and 2.8*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_d = {}\n",
    "\n",
    "tags = ['CC', 'IN', 'JJ']\n",
    "\n",
    "for t in tags:\n",
    "    for p in ma_pos:\n",
    "        if p == t:\n",
    "            ma_pos_d[t] = 1 + ma_pos_d.get(t, 0)\n",
    "            \n",
    "for t in tags:\n",
    "    ma_pos_d[t] = (ma_pos_d[t]/len(ma_pos)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again the respective results in Zhao & Zobel were 3.2, 6.4, and 2.4.  Obviously, a much different POS tagger was used.*\n",
    "\n",
    "*I was curious to know what would happen if I used Spearman's Correlation Coefficient on rankings of POS tags.  So I used a frequency distribution to create a ranking of the most commonly used POS tags for the Shakespeare and Marlowe corpora, and then calculated the correlation coefficient on the first test text: Shakespeare's \"Othello\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', \n",
    "            'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', \n",
    "            'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', '', 'UH', 'VB', \n",
    "            'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_fd = nltk.FreqDist(sh_pos)\n",
    "sh_tags_sorted = [l for r, l in sorted([(v, k) for k, v in sh_pos_fd.items()], reverse = True)]\n",
    "print(sh_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_fd = nltk.FreqDist(ma_pos)\n",
    "ma_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ma_pos_fd.items()], reverse = True)]\n",
    "print(ma_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho_file = sorted(glob.glob(os.path.join(path, \"Holdout1.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho = re.sub(k, v, ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_pos = [p[1] for p in nltk.pos_tag(ho)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_pos_fd = nltk.FreqDist(ho_pos)\n",
    "ho_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho_pos_fd.items()], reverse = True)]\n",
    "print(ho_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.spearman import *\n",
    "\n",
    "sh_sc = spearman_correlation(ranks_from_sequence(ho_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The two coefficients are very similar, but the coefficient for the Shakespeare corpus is higher, indicating the correct author.  I tried a similar experiment, with the rankings of function words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_function_fd = nltk.FreqDist(sh_function)\n",
    "sh_func_sorted = [l for r, l in sorted([(v, k) for k, v in sh_function_fd.items()], reverse = True)]\n",
    "print(sh_func_sorted[:10], end = '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_function_fd = nltk.FreqDist(ma_function)\n",
    "ma_func_sorted = [l for r, l in sorted([(v, k) for k, v in ma_function_fd.items()], reverse = True)]\n",
    "print(ma_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_words = word_tokenizer.tokenize(ho.lower())\n",
    "ho_function = [w for w in ho_words if w in function_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_function_fd = nltk.FreqDist(ho_function)\n",
    "ho_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho_function_fd.items()], reverse = True)]\n",
    "print(ho_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, the coefficient is higher when we compare \"Othello\" to the Shakespeare corpus; but the difference is much more noticeable here, suggesting that the ranking of function words is a much better predictor.*\n",
    "\n",
    "*We may have just been lucky, so let's repeat the experiment with Marlowe's \"The Massacre at Paris\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho2_file = sorted(glob.glob(os.path.join(path, \"Holdout2.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho2_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho2 = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho2 = re.sub(k, v, ho2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_pos = [p[1] for p in nltk.pos_tag(ho2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_pos_fd = nltk.FreqDist(ho2_pos)\n",
    "ho2_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho2_pos_fd.items()], reverse = True)]\n",
    "print(ho2_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_sc = spearman_correlation(ranks_from_sequence(ho2_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho2_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When considering ranks of POS tags, the correlation is again higher for the Shakespeare corpus, which is the wrong result i this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_words = word_tokenizer.tokenize(ho2.lower())\n",
    "ho2_function = [w for w in ho2_words if w in function_words]\n",
    "ho2_function_fd = nltk.FreqDist(ho2_function)\n",
    "ho2_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho2_function_fd.items()], reverse = True)]\n",
    "print(ho2_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho2_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho2_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However, once again the rankings of function words is correct, and by a comparatively large margin.*\n",
    "\n",
    "*Let's try it a third time, with \"Romeo & Juliet\":*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho3_file = sorted(glob.glob(os.path.join(path, \"Holdout3.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho3_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho3 = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho3 = re.sub(k, v, ho3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_pos = [p[1] for p in nltk.pos_tag(ho3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_pos_fd = nltk.FreqDist(ho3_pos)\n",
    "ho3_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho3_pos_fd.items()], reverse = True)]\n",
    "print(ho3_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_sc = spearman_correlation(ranks_from_sequence(ho3_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho3_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, the wrong answer for the POS-tag ranking correlation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_words = word_tokenizer.tokenize(ho3.lower())\n",
    "ho3_function = [w for w in ho3_words if w in function_words]\n",
    "ho3_function_fd = nltk.FreqDist(ho3_function)\n",
    "ho3_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho3_function_fd.items()], reverse = True)]\n",
    "print(ho3_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho3_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho3_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But comparing the ranking of function words again indicates the correct author, and by a decent-sized margin.*\\\n",
    "\n",
    "*This sample was quite small, but it does confirm some of the results of Zhao & Zobel - namely, the frequency of use of function words is a fairly reliable indicator of authorship.  Also interesting was the fact that fairly primitive methods were more accurate than complex machine learning predictors.  Sometimes, the best model might indeed be the simplest one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28. \n",
    "\n",
    "★ Study gender-specific lexical choice, and see if you can reproduce some of the results of ~~http://www.clintoneast.com/articles/words.php~~.\n",
    "\n",
    "*The link is dead (Long live the link!).  Since the author's of the NLTK book neglected to include any other information about this study, it's going to be impossible to try to locate it at another site.*\n",
    "\n",
    "##### 29. \n",
    "\n",
    "★ Write a recursive function that pretty prints a trie in alphabetically sorted order, e.g.:\n",
    "\n",
    "```\n",
    "chair: 'flesh'\n",
    "---t: 'cat'\n",
    "--ic: 'stylish'\n",
    "---en: 'dog'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'h': {'a': {'t': {'value': 'cat'}, 'i': {'r': {'value': 'flesh'}}},\n",
       "   'i': {'e': {'n': {'value': 'dog'}}, 'c': {'value': 'stylish'}}}}}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie = {}\n",
    "insert(trie, 'chat', 'cat')\n",
    "insert(trie, 'chien', 'dog')\n",
    "insert(trie, 'chair', 'flesh')\n",
    "insert(trie, 'chic', 'stylish')\n",
    "trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I had considerable difficulties with this and had to resort to asking people on Stack Overflow for [help](https://stackoverflow.com/questions/58653062/python-printing-out-a-trie-in-alphabetically-sorted-order-with-a-recursive-fun?answertab=votes#tab-top \"Stack Overflow discussion\").  I had made the mistake of always trying to use a return statement in a recursive function, which is not necessary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trie(trie, string = \"\"):\n",
    "    \"\"\"\n",
    "    Recursive function to print Trie entries in alphabetical order,\n",
    "    and replace common elements with dashes.\n",
    "    \"\"\"\n",
    "    flag = True\n",
    "    for key, value in sorted(trie.items(), key = lambda x: x[0]):\n",
    "    # dictionary sorted based upon the keys\n",
    "        if isinstance(value, dict):\n",
    "            if flag:\n",
    "                prefix = string + key         \n",
    "                # flag to show common prefix\n",
    "                flag = False\n",
    "            else:\n",
    "                # dashes for common prefix\n",
    "                prefix = '-' * len(string) + key  \n",
    "            # string + key is extending string  for print_trie \n",
    "            # by appending current key k\n",
    "            print_trie(value, prefix)   \n",
    "        else:\n",
    "            # not a dictionary, so print current string and value\n",
    "            print(string + \":\", value)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair: flesh\n",
      "---t: cat\n",
      "--ic: stylish\n",
      "---en: dog\n"
     ]
    }
   ],
   "source": [
    "# Create Trie\n",
    "trie = {}\n",
    "insert(trie, 'chat', 'cat')\n",
    "insert(trie, 'chien', 'dog')\n",
    "insert(trie, 'chair', 'flesh')\n",
    "insert(trie, 'chic', 'stylish')\n",
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 30. \n",
    "\n",
    "★ With the help of the trie data structure, write a recursive function that processes text, locating the uniqueness point in each word, and discarding the remainder of each word. How much compression does this give? How readable is the resulting text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trie_dict(trie, string = \"\", d = {}):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses a trie to create a dictionary of redacted values\n",
    "    \"\"\"\n",
    "    flag = True\n",
    "    for key, value in sorted(trie.items(), key = lambda x: x[0]):\n",
    "        \n",
    "        # dictionary sorted based upon the keys\n",
    "        if isinstance(value, dict):\n",
    "            if flag:\n",
    "                prefix = string + key\n",
    "                flag = False\n",
    "            else:\n",
    "                prefix = '-' * len(string) + key\n",
    "            make_trie_dict(value, prefix, d)\n",
    "\n",
    "        else:\n",
    "            d[value] = string\n",
    "            \n",
    "    return d\n",
    "\n",
    "def convert_text_to_trie(text):\n",
    "    # Normalize and tokenize text\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'’', \"'\", text)\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+['-]?\\w*\")\n",
    "\n",
    "    # get a set of words and make a trie from it\n",
    "    text_list = list(set([w.lower() for w in word_tokenizer.tokenize(text)]))\n",
    "    \n",
    "    trie = {}\n",
    "    for t in text_list:\n",
    "        insert(trie, t, t)\n",
    "        \n",
    "    return trie\n",
    "\n",
    "def compress_text(text, trie = {}):\n",
    "    \"\"\"\n",
    "    Converts a text into a version where the common elements\n",
    "    of the beginnings of words have been replaced with dashes.\n",
    "    \"\"\"\n",
    "\n",
    "    trie = convert_text_to_trie(text)\n",
    "    word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+['-]?\\w*|[,.:;?!()\\'\\\"\\[\\]&]\")\n",
    "    \n",
    "    # make a dictionary of the compressed values\n",
    "    comp_d = make_trie_dict(trie)\n",
    "\n",
    "    # return a new version with the compressed words\n",
    "    \n",
    "    new_text = []\n",
    "    for w in word_tokenizer.tokenize(text):\n",
    "        # look up lower-case variations of words\n",
    "        if w.lower() in comp_d:\n",
    "            # if original is in uppercase and first character is a letter\n",
    "            # the replacement word should also be uppercased in the new text\n",
    "            if w.isupper() and comp_d[w.lower()][0].isalpha():\n",
    "                new_text.append(comp_d[w.lower()].upper())\n",
    "            else:\n",
    "                new_text.append(comp_d[w.lower()])\n",
    "        # for strings not in the trie dictionary\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    \n",
    "    # join text and join punctuation to preceding words\n",
    "    return ' '.join(join_punctuation(new_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://medium.com/basecs/trying-to-understand-tries-3ec6bede0014\n",
    "\n",
    "text = \"\"\" A trie is a tree-like data structure whose nodes store the letters of an \n",
    "alphabet. By structuring the nodes in a particular way, words and strings can \n",
    "be retrieved from the structure by traversing down a branch path of the tree.\n",
    "Tries in the context of computer science are a relatively new thing. The first \n",
    "time that they were considered in computing was back in 1959, when a Frenchman \n",
    "named René de la Briandais suggested using them. According to Donald Knuth’s \n",
    "research in The Art of Computer Programming:\n",
    "Trie memory for computer searching was first recommended by René de la \n",
    "Briandais. He pointed out that we can save memory space at the expense of \n",
    "running time if we use a linked list for each node vector, since most of the \n",
    "entries in the vectors tend to be empty.\n",
    "The original idea behind using tries as a computing structure was that they \n",
    "could be a nice compromise between running time and memory. But we’ll come \n",
    "back to that in a bit. First, let’s take a step back and try and understand \n",
    "what exactly this structure looks like to start.\n",
    "\n",
    "The size of a trie correlates to the size of the alphabet it represents.\n",
    "We know that tries are often used to represent words in an alphabet. In the \n",
    "illustration shown here, we can start to get a sense of how exactly that \n",
    "representation works.\n",
    "Each trie has an empty root node, with links or references to other nodes — \n",
    "one for each possible alphabetic value.\n",
    "The shape and the structure of a trie is always a set of linked nodes, \n",
    "connecting back to an empty root node. An important thing to note is that \n",
    "the number of child nodes in a trie depends completely upon the total number \n",
    "of values possible. For example, if we are representing the English alphabet, \n",
    "then the total number of child nodes is directly connected to the total number \n",
    "of letters possible. In the English alphabet, there are 26 letters, so the \n",
    "total number of child nodes will be 26.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A --ie -s a --ee-like data ---ucture --ose -odes --ore --e ---ters of -n -lphabet. -y --------ing --e -odes -n a particular --y, -ords -nd --rings can -e --trieved --om --e ---ucture -y -raversing --wn a -ranch --th of --e --ee. --ies -n --e ---text of ----uter -cience -re a --latively -ew --ing. --e first -ime -hat ---y --re ---sidered -n ------ing was back -n 1959, --en a -renchman named --né -e la --iandais -uggested --ing --em. according -o -onald Knuth s --search -n --e --t of ----uter -rogramming: --ie memory -or ----uter -earching was first recommended -y --né -e la --iandais. -e -ointed -ut -hat -e can save memory -pace -t --e --pense of -unning -ime -f -e -se a --nked --st -or each -ode -ector, -ince -ost of --e --tries -n --e -ectors -end -o -e -mpty. --e -riginal idea -ehind --ing --ies -s a ------ing ---ucture was -hat ---y --uld -e a -ice ----romise --tween -unning -ime -nd memory. -ut -e ll -ome back -o -hat -n a -it. first, let s take a --ep back -nd --y -nd understand -hat -xactly ---s ---ucture -ooks -ike -o -tart. --e --ze of a --ie --rrelates -o --e --ze of --e -lphabet -t ---------s. -e know -hat --ies -re often -sed -o --present -ords -n -n -lphabet. -n --e -llustration --own -ere, -e can -tart -o get a --nse of -ow -xactly -hat --presentation ---ks. each --ie has -n -mpty -oot -ode, --th ----s -r --ferences -o -ther -odes -ne -or each --ssible -lphabetic value. --e -hape -nd --e ---ucture of a --ie -s --ways a --t of --nked -odes, -------ing back -o -n -mpty -oot -ode. -n -mportant --ing -o --te -s -hat --e -umber of -hild -odes -n a --ie -epends ---pletely -pon --e -otal -umber of values --ssible. -or ---mple, -f -e -re ---------ing --e -nglish -lphabet, ---n --e -otal -umber of -hild -odes -s -irectly --nnected -o --e -otal -umber of ---ters --ssible. -n --e -nglish -lphabet, ---re -re 26 ---ters, -o --e -otal -umber of -hild -odes -ill -e 26.'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compress_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The new text is quite difficult to read.  The characters that hold the most information (if you would) are the first and the last ones.  Without the initial characters the words are very difficult to read indeed.  With a short text, the problem is not so acute; but the longer the text, the more possibilities for compression, and the more illegible the final result.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 31. \n",
    "\n",
    "★ Obtain some raw text, in the form of a single, long string. Use Python's `textwrap` module to break it up into multiple lines. Now write code to add extra spaces between words, in order to justify the output. Each line must have the same width, and spaces must be approximately evenly distributed across each line. No line can begin or end with a space.\n",
    "\n",
    "*Using same text as I used for 4.30.  Normalizing the text:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" A trie is a tree-like data structure whose nodes store the letters of an alphabet. By structuring the nodes in a particular way, words and strings can be retrieved from the structure by traversing down a branch path of the tree. Tries in the context of computer science are a relatively new thing. The first time that they were considered in computing was back in 1959, when a Frenchman named René de la Briandais suggested using them. According to Donald Knuth's research in The Art of Computer Programming: Trie memory for computer searching was first recommended by René de la Briandais. He pointed out that we can save memory space at the expense of running time if we use a linked list for each node vector, since most of the entries in the vectors tend to be empty. The original idea behind using tries as a computing structure was that they could be a nice compromise between running time and memory. But we'll come back to that in a bit. First, let's take a step back and try and understand what exactly this structure looks like to start. The size of a trie correlates to the size of the alphabet it represents. We know that tries are often used to represent words in an alphabet. In the illustration shown here, we can start to get a sense of how exactly that representation works. Each trie has an empty root node, with links or references to other nodes — one for each possible alphabetic value. The shape and the structure of a trie is always a set of linked nodes, connecting back to an empty root node. An important thing to note is that the number of child nodes in a trie depends completely upon the total number of values possible. For example, if we are representing the English alphabet, then the total number of child nodes is directly connected to the total number of letters possible. In the English alphabet, there are 26 letters, so the total number of child nodes will be 26. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'\\n', ' ', text)\n",
    "text = re.sub(r' +', ' ', text)\n",
    "text = re.sub(r'’', \"'\", text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I wasn't that comfortable with `textwrap`, and because I felt that the module had been somewhat glossed over in this chapter, I went online to find examples of the module in action and came across a function [here](http://code.activestate.com/recipes/414870-align-text-string-using-spaces-between-words-to-fi/ \"Align text\") very similar to the one the authors are asking for.  The function below is heavily influence by that code, but I made a number of changes: 1) instead of padding the text by adding spaces to the words at the beginning of a line, I added spaces to random words; 2) all lines are distributed evenly in my function, whereas the function at the link gave the option to align the last line; 3) I simplified the function considerably: the original function worked on a variety of data types, but mine only works on strings.  The original function also had a number of debugging features that I left out to keep the function simple.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, textwrap, random\n",
    "\n",
    "def get_length(items):\n",
    "    \"\"\"\n",
    "    Find total length of strings inside a list,\n",
    "    including spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum([len(i) for i in items])\n",
    "\n",
    "\n",
    "def distribute_paragraph(text, width):\n",
    "    \"\"\"\n",
    "    Add spaces to words in text so that\n",
    "    each line is printed with a uniform width\n",
    "    \"\"\"\n",
    "    splitted = textwrap.wrap(' '.join([text]), width) \n",
    "\n",
    "    for line in splitted:\n",
    "              \n",
    "        items = line.split()\n",
    "    \n",
    "        # evenly add spaces to each item (except the last item)\n",
    "        # until the difference between the new length of the line \n",
    "        # and the width is zero, or less than the number of items\n",
    "        for j in range((width - get_length(items)) // len(items)):\n",
    "            for i in range(len(items) - 1):\n",
    "                items[i] += ' '\n",
    "\n",
    "        # add spaces to random items until width is reached\n",
    "        for si in random.sample(range(0, len(items) - 1), \n",
    "                                width - get_length(items)):\n",
    "            items[si] = items[si] + ' '\n",
    "\n",
    "        print(''.join(items))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A trie  is a  tree-like data structure whose nodes store the\n",
      "letters  of  an  alphabet. By  structuring  the nodes  in  a\n",
      "particular way,  words and strings can be retrieved from the\n",
      "structure  by traversing  down  a branch  path  of the tree.\n",
      "Tries in the context  of computer science  are  a relatively\n",
      "new  thing.  The first time  that  they were  considered  in\n",
      "computing was back in 1959, when  a  Frenchman named René de\n",
      "la  Briandais  suggested  using them.  According  to  Donald\n",
      "Knuth's research in The  Art of Computer  Programming:  Trie\n",
      "memory for computer  searching was first recommended by René\n",
      "de  la  Briandais.  He pointed  out that we can save  memory\n",
      "space at the expense of running time if we use a linked list\n",
      "for  each  node vector, since  most  of  the  entries in the\n",
      "vectors  tend to  be empty. The  original idea  behind using\n",
      "tries as a computing structure was that they could be a nice\n",
      "compromise between  running time and  memory. But we'll come\n",
      "back to that in a bit. First, let's take a step back and try\n",
      "and understand what  exactly  this structure looks  like  to\n",
      "start.  The  size of  a trie  correlates to  the size of the\n",
      "alphabet it represents. We know that tries are often used to\n",
      "represent words in  an alphabet. In  the illustration  shown\n",
      "here,  we  can  start  to get  a sense  of  how exactly that\n",
      "representation works. Each trie has an empty root node, with\n",
      "links or  references to other nodes — one  for each possible\n",
      "alphabetic  value. The shape and the structure of a  trie is\n",
      "always a set of  linked nodes, connecting back to  an  empty\n",
      "root node. An important  thing to note is that the number of\n",
      "child  nodes in  a  trie depends completely  upon the  total\n",
      "number   of  values   possible.  For  example,  if   we  are\n",
      "representing the English alphabet, then  the total number of\n",
      "child nodes  is directly connected  to  the  total number of\n",
      "letters  possible.  In the  English alphabet,  there  are 26\n",
      "letters,  so the  total number  of  child nodes will be  26.\n"
     ]
    }
   ],
   "source": [
    "distribute_paragraph(text, width = 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 32. \n",
    "\n",
    "★ Develop a simple extractive summarization tool, that prints the sentences of a document which contain the highest total word frequency. Use `FreqDist()` to count word frequencies, and use sum to sum the frequencies of the words in each sentence. Rank the sentences according to their score. Finally, print the $n$ highest-scoring sentences in document order. Carefully review the design of your program, especially your approach to this double sorting. Make sure the program is written as clearly as possible.\n",
    "\n",
    "*I'm again a little confused about the wording of this problem.  It sounds like I'm supposed to rank sentences based upon how many common words they have, meaning that long sentences with lots of common words would have the highest scores.  As coding exercise, I can understand the point of this; but not really from a linguistic perspective, since common words generally carry the least meaning.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def extract_summaries(text, n):\n",
    "    \"\"\"\n",
    "    Tokenize sentences in a text and rank these sentences based\n",
    "    on the sum of the frequency scores of their constituent\n",
    "    words.  Return the top n sentences and their scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    sents = sentence_tokenizer.tokenize(text)\n",
    "    \n",
    "    # frequencies of tokenized words\n",
    "    fd = nltk.FreqDist(tokenizer.tokenize(text))   \n",
    "        \n",
    "    # create scores and sort a list based on these scores\n",
    "    top_n = list(sorted(set([(sum([fd[w.lower()] for w in s]), s) for s in sents]), reverse = True))\n",
    "    \n",
    "    # return top n scores\n",
    "    return [(top_n[i][1], \"Score: {}\".format(top_n[i][0])) for i in range(n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('By structuring the nodes in a particular way, words and strings can be retrieved from the structure by traversing down a branch path of the tree.',\n",
       "  'Score: 164'),\n",
       " (\"According to Donald Knuth's research in The Art of Computer Programming: Trie memory for computer searching was first recommended by René de la Briandais.\",\n",
       "  'Score: 162'),\n",
       " (\"First, let's take a step back and try and understand what exactly this structure looks like to start.\",\n",
       "  'Score: 151'),\n",
       " ('The first time that they were considered in computing was back in 1959, when a Frenchman named René de la Briandais suggested using them.',\n",
       "  'Score: 149'),\n",
       " ('The shape and the structure of a trie is always a set of linked nodes, connecting back to an empty root node.',\n",
       "  'Score: 132')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summaries(text, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 33.\n",
    "\n",
    "★ Read the following article on semantic orientation of adjectives. Use the NetworkX package to visualize a network of adjectives with edges to indicate same vs different semantic orientation. http://www.aclweb.org/anthology/P97-1023\n",
    "\n",
    "*The problem is neither especially interesting or useful for me now, so I think I'll skip it.  At the very least, while doing initial research for this question I came across `sentiwordnet`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<breakdown.n.03: PosScore=0.0 NegScore=0.25>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "breakdown = swn.senti_synset('breakdown.n.03')\n",
    "print(breakdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 34. \n",
    "\n",
    "★ Design an algorithm to find the \"statistically improbable phrases\" of a document collection. ~~http://www.amazon.com/gp/search-inside/sipshelp.html~~ *This link as well is dead.*\n",
    "\n",
    "*My idea was to construct some sort of tf-idf calculator for the bigrams in a corpus.  I tried this with the corpus I had most ready access to - the Brown Corpus - but since the Corpus is on the small side (just over 1m words), the number of bigrams represented is also fairly small (ca. 430k).  Ergo, the most statistically improbable phrases in a document may not be all that improbable, since the corpus is too small to be a good representation for probable phrases in English.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "import math\n",
    "\n",
    "# make a list of all the bigrams in the corpus\n",
    "brown_bigrams = [i for f in brown.fileids() for s in brown.sents(f) for i in list(nltk.bigrams([w.lower() for w in s]))]\n",
    "\n",
    "# create a FreqDist of bigram counts in the corpus\n",
    "bbfd = nltk.FreqDist(brown_bigrams)\n",
    "\n",
    "def find_brown_sip(doc, n):\n",
    "\n",
    "    # a list of all the bigrams in the test document\n",
    "    bigram_x = [i for s in brown.sents(doc) for i in list(nltk.bigrams([w.lower() for w in s]))]\n",
    "\n",
    "    # FreqDist of bigrams in the test document\n",
    "    fd = nltk.FreqDist(bigram_x)\n",
    "\n",
    "    # make a list of TF/IDF scores for each bigram in the test document\n",
    "    tfidf_scores = []\n",
    "\n",
    "    for bx in bigram_x:\n",
    "        tf =  fd[bx] / len(bigram_x) \n",
    "        idf = math.log(len(brown.fileids())/bbfd[bx])\n",
    "        tfidf_scores.append(((tf * idf), bx))\n",
    "\n",
    "    # print the ten highest scoring bigrams\n",
    "    return sorted(set(tfidf_scores), reverse = True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.014637757722981792, (\"''\", ',', 'moreland')),\n",
       " (0.011742076006710098, ('humor', 'and', 'comedy')),\n",
       " (0.011458152068708767, (\"''\", ',', 'i')),\n",
       " (0.00933215211841978, ('i', 'said', '.')),\n",
       " (0.00923376983794013, (',', 'i', 'said')),\n",
       " (0.006923934522881674, (',', 'and', 'said')),\n",
       " (0.006791144880204534, ('believe', 'in', 'the')),\n",
       " (0.006791144880204534, ('and', 'comedy', \"''\")),\n",
       " (0.006791144880204534, (',', 'moreland', 'said')),\n",
       " (0.006409266022967657, ('drink', ',', 'and'))]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x = brown.fileids()[random.randint(0, 500)]\n",
    "print(x)\n",
    "find_brown_sip(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Repeating the above function with trigrams instead of bigrams:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "import math\n",
    "\n",
    "# make a list of all the trigrams in the corpus\n",
    "brown_trigrams = [i for f in brown.fileids() for s in brown.sents(f) for i in list(nltk.ngrams([w.lower() for w in s], 3))]\n",
    "\n",
    "# create a FreqDist of trigram counts in the corpus\n",
    "bbtd = nltk.FreqDist(brown_trigrams)\n",
    "\n",
    "def find_brown_sip3(doc, n):\n",
    "\n",
    "    # a list of all the trigrams in the test document\n",
    "    trigram_x = [i for s in brown.sents(doc) for i in list(nltk.ngrams([w.lower() for w in s], 3))]\n",
    "\n",
    "    # FreqDist of trigrams in the test document\n",
    "    fd = nltk.FreqDist(trigram_x)\n",
    "\n",
    "    # make a list of TF/IDF scores for each trigram in the test document\n",
    "    tfidf_scores = []\n",
    "\n",
    "    for bx in trigram_x:\n",
    "        tf =  fd[bx] / len(trigram_x) \n",
    "        idf = math.log(len(brown.fileids())/bbtd[bx])\n",
    "        tfidf_scores.append(((tf * idf), bx))\n",
    "\n",
    "    # print the ten highest scoring trigrams\n",
    "    return sorted(set(tfidf_scores), reverse = True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.010456789704786766, ('in', 'the', 'refrigerator')),\n",
       " (0.010042798885545271, ('the', 'refrigerator', ',')),\n",
       " (0.008770778814354771, ('hand', 'lotion', '.')),\n",
       " (0.008365431763829412, (\"i've\", 'got', 'this')),\n",
       " (0.007106308820032962, (',', 'only', 'to')),\n",
       " (0.006970021539174499, ('mrs.', 'kirby', ',')),\n",
       " (0.006970021539174499, ('got', 'this', 'cold')),\n",
       " (0.005515993220843317, ('up', 'and', 'down')),\n",
       " (0.005473274551888245, ('on', 'the', 'beach')),\n",
       " (0.005329731615024722, ('but', 'then', ','))]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "x = brown.fileids()[random.randint(0, 500)]\n",
    "print(x)\n",
    "find_brown_sip3(x, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 35.\n",
    "\n",
    "★ Write a program to implement a brute-force algorithm for discovering word squares, a kind of $n \\times n$ crossword in which the entry in the $n$th row is the same as the entry in the $n$th column. For discussion, see http://itre.cis.upenn.edu/~myl/languagelog/archives/002679.html\n",
    "\n",
    "*This is a problem I tackled earlier, so my solution is below.  This solution uses dictionaries, which are sufficiently fast.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"C:\\\\Users\\\\matth\\\\Desktop\\\\ThinkJulia\"\n",
    "os.chdir(path)\n",
    "fin = open('words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using the wordlist from the book [__Think Python 2e__](http://www.greenteapress.com/thinkpython2/html/thinkpython2010.html \"Think Python 2e\") by Allen Downey.  Wordlist available [here](http://greenteapress.com/thinkpython2/code/words.txt \"words.txt\").*\n",
    "\n",
    "*Below is the non-recursive version of code which finds solutions for the 4x4 puzzle:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "def make_fourletter_wordlist():\n",
    "    \"\"\"\n",
    "    Reads lines from word.txt and \n",
    "    makes a list using append.\n",
    "    \"\"\"\n",
    "    fin = open('words.txt')\n",
    "    t = []\n",
    "\n",
    "    for line in fin:   \n",
    "        word = line.strip()\n",
    "        if len(word) == 4:\n",
    "            t.append(word) \n",
    "    return t\n",
    "\n",
    "fw = make_fourletter_wordlist()\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "d = {}\n",
    "\n",
    "for word in fw:\n",
    "    for i in range(4):\n",
    "        d.setdefault(word[0:i], []).append(word)\n",
    "        \n",
    "word_squares = []\n",
    "\n",
    "for w in fw:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in d:\n",
    "        for cand1 in d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in d:\n",
    "                for cand2 in d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in d:\n",
    "                        for cand3 in d[check3]:\n",
    "                            word_squares.append([test, cand1, cand2, cand3])\n",
    "                            \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126574\n",
      "Total elapsed time for the non-recursive version of the 4x4 puzzle: 5.3598 seconds\n"
     ]
    }
   ],
   "source": [
    "print(len(word_squares))\n",
    "print(\"Total elapsed time for the non-recursive version of the 4x4 puzzle: {:.4f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below is a recursive version of the code which can be used with word squares of any size:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_n_letter_wordlist(text, n):\n",
    "    \"\"\"\n",
    "    Reads lines from a raw text file with one\n",
    "    word per line and makes a list of those\n",
    "    words having length n.\n",
    "    \"\"\"\n",
    "    #fin = open('words.txt')\n",
    "    fin = open(text)\n",
    "    t = []\n",
    "\n",
    "    for line in fin:   \n",
    "        word = line.strip()\n",
    "        if len(word) == n:\n",
    "            t.append(word) \n",
    "    return t\n",
    "\n",
    "def make_n_minus1_dict(wordlist, n):\n",
    "    \"\"\"\n",
    "    makes dictionaries of first 1:n-1 letters of each word in a word list.\n",
    "    \"\"\"\n",
    "\n",
    "    d = {}\n",
    "\n",
    "    for word in wordlist:\n",
    "        for i in range(1, n):\n",
    "            d.setdefault(word[0:i], []).append(word)\n",
    "            \n",
    "    return d\n",
    "\n",
    "def check_dict(d, word):\n",
    "    \"\"\"\n",
    "    Returns dictionary values if the jth letters\n",
    "    of a list of words of size j are keys in dictionary d.\n",
    "    \n",
    "    Arguments:\n",
    "    d:      Specially-contrived dictionary with all combinations\n",
    "            of the first 1:n-1 letters of the word list used\n",
    "            to make the dictionary\n",
    "    word:   list of up to n-1 words\n",
    "    \"\"\"\n",
    "    i = len(word)\n",
    "    key = ''.join(w[i] for w in word)\n",
    "    if key in d:\n",
    "        return [word, d[key]]\n",
    "\n",
    "def recursive_ws(d, words, ws, n):\n",
    "    \"\"\"\n",
    "    Recursively looks for all possible word squares\n",
    "    for the first word in list words.\n",
    "    \n",
    "    Arguments:\n",
    "    d:      Specially-contrived dictionary with all combinations\n",
    "            of the first 1:n-1 letters of the word list used\n",
    "            to make the dictionary\n",
    "    word:   list of up to n-1 words\n",
    "    ws:     list of completed word squares\n",
    "    n:      number of letters in one side of the word square.\n",
    "    \"\"\"\n",
    "    \n",
    "    check = check_dict(d, words)\n",
    "    if check:           \n",
    "        \n",
    "        # add completed square to list of completed squares\n",
    "        if len(check[0]) == n - 1:\n",
    "            for cand in check[1]:\n",
    "                words = check[0].copy()\n",
    "                words.append(cand)\n",
    "                ws.append(words)\n",
    "        else:\n",
    "            # evaluate other possible candidates\n",
    "            for cand in check[1]:\n",
    "                words = check[0].copy()\n",
    "                words.append(cand)\n",
    "\n",
    "                recursive_ws(d, words, ws, n)\n",
    "                \n",
    "def make_word_square(text, n):\n",
    "    \"\"\"\n",
    "    Finds all possible word squares of size n x n from \n",
    "    a list of words of length n in text.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = make_n_letter_wordlist('words.txt', n)\n",
    "    nd = make_n_minus1_dict(word_list, n)\n",
    "    \n",
    "    ws = []\n",
    "    \n",
    "    for w in word_list:\n",
    "        words = [w]\n",
    "        recursive_ws(nd, words, ws, n)\n",
    "        \n",
    "    return ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These two programs are self-explanatory:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def print_random_ws(ws):\n",
    "    \"\"\"\n",
    "    Prints out a random word square from a list of word squares.\n",
    "    \"\"\"\n",
    "    random_ws = ws[random.randint(0, len(ws))]\n",
    "    for word in random_ws:\n",
    "        print(word.upper())\n",
    "        \n",
    "def find_word_in_word_squares(ws, word):\n",
    "    \"\"\"\n",
    "    Finds all words squares ws with word.\n",
    "    \"\"\"\n",
    "    for w in ws:\n",
    "        if word in w:\n",
    "            print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126574\n",
      "Total elapsed time for the recursive version of the 4x4 puzzle: 11.9198 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "four_word_square = make_word_square('words.txt', 4)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(len(four_word_square))\n",
    "print(\"Total elapsed time for the recursive version of the 4x4 puzzle: {:.4f} seconds\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATS\n",
      "AYAH\n",
      "TAPE\n",
      "SHED\n"
     ]
    }
   ],
   "source": [
    "print_random_ws(four_word_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acme', 'caid', 'mind', 'eddo']\n",
      "['acme', 'caid', 'mind', 'eddy']\n",
      "['acme', 'cain', 'mind', 'ends']\n",
      "['acme', 'ceil', 'mind', 'elds']\n",
      "['acme', 'chid', 'mind', 'eddo']\n",
      "['acme', 'chid', 'mind', 'eddy']\n",
      "['acme', 'chin', 'mind', 'ends']\n",
      "['acme', 'coil', 'mind', 'elds']\n",
      "['acme', 'coin', 'mind', 'ends']\n",
      "['agma', 'gain', 'mind', 'ands']\n",
      "['agma', 'grid', 'mind', 'adds']\n",
      "['agma', 'grin', 'mind', 'ands']\n",
      "['agma', 'guid', 'mind', 'adds']\n",
      "['ahem', 'hili', 'elan', 'mind']\n",
      "['aims', 'ilia', 'mind', 'sade']\n",
      "['aims', 'ilia', 'mind', 'sadi']\n",
      "['aims', 'inia', 'mind', 'sade']\n",
      "['aims', 'inia', 'mind', 'sadi']\n",
      "['aims', 'ixia', 'mind', 'sade']\n",
      "['aims', 'ixia', 'mind', 'sadi']\n",
      "['alma', 'laid', 'mind', 'adds']\n",
      "['alma', 'lain', 'mind', 'ands']\n",
      "['alma', 'loin', 'mind', 'ands']\n",
      "['alme', 'laid', 'mind', 'eddo']\n",
      "['alme', 'laid', 'mind', 'eddy']\n",
      "['alme', 'lain', 'mind', 'ends']\n",
      "['alme', 'loin', 'mind', 'ends']\n",
      "['amia', 'mind', 'info', 'ados']\n",
      "['amia', 'mind', 'into', 'ados']\n",
      "['amie', 'mind', 'inch', 'edhs']\n",
      "['ammo', 'maid', 'mind', 'odds']\n",
      "['ammo', 'mail', 'mind', 'olds']\n",
      "['ammo', 'mair', 'mind', 'ordo']\n",
      "['ammo', 'moil', 'mind', 'olds']\n",
      "['arms', 'raia', 'mind', 'sade']\n",
      "['arms', 'raia', 'mind', 'sadi']\n",
      "['atma', 'tain', 'mind', 'ands']\n",
      "['atma', 'thin', 'mind', 'ands']\n",
      "['atma', 'twin', 'mind', 'ands']\n",
      "['atom', 'taxi', 'oxen', 'mind']\n",
      "['atom', 'tipi', 'open', 'mind']\n",
      "['atom', 'topi', 'open', 'mind']\n",
      "['berm', 'etui', 'ruin', 'mind']\n",
      "['bima', 'imid', 'mind', 'adds']\n",
      "['bomb', 'obia', 'mind', 'bade']\n",
      "['bomb', 'obia', 'mind', 'bads']\n",
      "['bomb', 'ohia', 'mind', 'bade']\n",
      "['bomb', 'ohia', 'mind', 'bads']\n",
      "['bomb', 'olio', 'mind', 'bode']\n",
      "['bomb', 'olio', 'mind', 'bods']\n",
      "['bomb', 'olio', 'mind', 'body']\n",
      "['brim', 'raki', 'ikon', 'mind']\n",
      "['came', 'acid', 'mind', 'eddo']\n",
      "['came', 'acid', 'mind', 'eddy']\n",
      "['came', 'agin', 'mind', 'ends']\n",
      "['came', 'akin', 'mind', 'ends']\n",
      "['came', 'amid', 'mind', 'eddo']\n",
      "['came', 'amid', 'mind', 'eddy']\n",
      "['came', 'amin', 'mind', 'ends']\n",
      "['came', 'anil', 'mind', 'elds']\n",
      "['came', 'arid', 'mind', 'eddo']\n",
      "['came', 'arid', 'mind', 'eddy']\n",
      "['came', 'aril', 'mind', 'elds']\n",
      "['came', 'avid', 'mind', 'eddo']\n",
      "['came', 'avid', 'mind', 'eddy']\n",
      "['came', 'axil', 'mind', 'elds']\n",
      "['came', 'ayin', 'mind', 'ends']\n",
      "['camp', 'agio', 'mind', 'pods']\n",
      "['camp', 'amia', 'mind', 'pads']\n",
      "['camp', 'amie', 'mind', 'peds']\n",
      "['camp', 'aria', 'mind', 'pads']\n",
      "['cams', 'agio', 'mind', 'soda']\n",
      "['cams', 'agio', 'mind', 'sods']\n",
      "['cams', 'amia', 'mind', 'sade']\n",
      "['cams', 'amia', 'mind', 'sadi']\n",
      "['cams', 'aria', 'mind', 'sade']\n",
      "['cams', 'aria', 'mind', 'sadi']\n",
      "['cham', 'hili', 'alan', 'mind']\n",
      "['chum', 'hili', 'ulan', 'mind']\n",
      "['coma', 'oxid', 'mind', 'adds']\n",
      "['comb', 'obia', 'mind', 'bade']\n",
      "['comb', 'obia', 'mind', 'bads']\n",
      "['comb', 'ohia', 'mind', 'bade']\n",
      "['comb', 'ohia', 'mind', 'bads']\n",
      "['comb', 'olio', 'mind', 'bode']\n",
      "['comb', 'olio', 'mind', 'bods']\n",
      "['comb', 'olio', 'mind', 'body']\n",
      "['come', 'oxid', 'mind', 'eddo']\n",
      "['come', 'oxid', 'mind', 'eddy']\n",
      "['comp', 'obia', 'mind', 'pads']\n",
      "['comp', 'ohia', 'mind', 'pads']\n",
      "['comp', 'olio', 'mind', 'pods']\n",
      "['cram', 'ragi', 'agin', 'mind']\n",
      "['cram', 'ragi', 'agon', 'mind']\n",
      "['cram', 'raki', 'akin', 'mind']\n",
      "['cram', 'rami', 'amen', 'mind']\n",
      "['cram', 'rami', 'amin', 'mind']\n",
      "['cram', 'rani', 'anon', 'mind']\n",
      "['culm', 'unai', 'lain', 'mind']\n",
      "['culm', 'unai', 'lawn', 'mind']\n",
      "['dame', 'acid', 'mind', 'eddo']\n",
      "['dame', 'acid', 'mind', 'eddy']\n",
      "['dame', 'agin', 'mind', 'ends']\n",
      "['dame', 'akin', 'mind', 'ends']\n",
      "['dame', 'amid', 'mind', 'eddo']\n",
      "['dame', 'amid', 'mind', 'eddy']\n",
      "['dame', 'amin', 'mind', 'ends']\n",
      "['dame', 'anil', 'mind', 'elds']\n",
      "['dame', 'arid', 'mind', 'eddo']\n",
      "['dame', 'arid', 'mind', 'eddy']\n",
      "['dame', 'aril', 'mind', 'elds']\n",
      "['dame', 'avid', 'mind', 'eddo']\n",
      "['dame', 'avid', 'mind', 'eddy']\n",
      "['dame', 'axil', 'mind', 'elds']\n",
      "['dame', 'ayin', 'mind', 'ends']\n",
      "['damn', 'agio', 'mind', 'node']\n",
      "['damn', 'agio', 'mind', 'nodi']\n",
      "['damn', 'agio', 'mind', 'nods']\n",
      "['damp', 'agio', 'mind', 'pods']\n",
      "['damp', 'amia', 'mind', 'pads']\n",
      "['damp', 'amie', 'mind', 'peds']\n",
      "['damp', 'aria', 'mind', 'pads']\n",
      "['dams', 'agio', 'mind', 'soda']\n",
      "['dams', 'agio', 'mind', 'sods']\n",
      "['dams', 'amia', 'mind', 'sade']\n",
      "['dams', 'amia', 'mind', 'sadi']\n",
      "['dams', 'aria', 'mind', 'sade']\n",
      "['dams', 'aria', 'mind', 'sadi']\n",
      "['deme', 'evil', 'mind', 'elds']\n",
      "['demo', 'emir', 'mind', 'ordo']\n",
      "['demo', 'evil', 'mind', 'olds']\n",
      "['derm', 'etui', 'ruin', 'mind']\n",
      "['dime', 'imid', 'mind', 'eddo']\n",
      "['dime', 'imid', 'mind', 'eddy']\n",
      "['dims', 'ilia', 'mind', 'sade']\n",
      "['dims', 'ilia', 'mind', 'sadi']\n",
      "['dims', 'inia', 'mind', 'sade']\n",
      "['dims', 'inia', 'mind', 'sadi']\n",
      "['dims', 'ixia', 'mind', 'sade']\n",
      "['dims', 'ixia', 'mind', 'sadi']\n",
      "['dome', 'oxid', 'mind', 'eddo']\n",
      "['dome', 'oxid', 'mind', 'eddy']\n",
      "['doms', 'obia', 'mind', 'sade']\n",
      "['doms', 'obia', 'mind', 'sadi']\n",
      "['doms', 'ohia', 'mind', 'sade']\n",
      "['doms', 'ohia', 'mind', 'sadi']\n",
      "['doms', 'olio', 'mind', 'soda']\n",
      "['doms', 'olio', 'mind', 'sods']\n",
      "['dram', 'ragi', 'agin', 'mind']\n",
      "['dram', 'ragi', 'agon', 'mind']\n",
      "['dram', 'raki', 'akin', 'mind']\n",
      "['dram', 'rami', 'amen', 'mind']\n",
      "['dram', 'rami', 'amin', 'mind']\n",
      "['dram', 'rani', 'anon', 'mind']\n",
      "['fame', 'acid', 'mind', 'eddo']\n",
      "['fame', 'acid', 'mind', 'eddy']\n",
      "['fame', 'agin', 'mind', 'ends']\n",
      "['fame', 'akin', 'mind', 'ends']\n",
      "['fame', 'amid', 'mind', 'eddo']\n",
      "['fame', 'amid', 'mind', 'eddy']\n",
      "['fame', 'amin', 'mind', 'ends']\n",
      "['fame', 'anil', 'mind', 'elds']\n",
      "['fame', 'arid', 'mind', 'eddo']\n",
      "['fame', 'arid', 'mind', 'eddy']\n",
      "['fame', 'aril', 'mind', 'elds']\n",
      "['fame', 'avid', 'mind', 'eddo']\n",
      "['fame', 'avid', 'mind', 'eddy']\n",
      "['fame', 'axil', 'mind', 'elds']\n",
      "['fame', 'ayin', 'mind', 'ends']\n",
      "['feme', 'evil', 'mind', 'elds']\n",
      "['from', 'rami', 'omen', 'mind']\n",
      "['gamb', 'agio', 'mind', 'bode']\n",
      "['gamb', 'agio', 'mind', 'bods']\n",
      "['gamb', 'agio', 'mind', 'body']\n",
      "['gamb', 'amia', 'mind', 'bade']\n",
      "['gamb', 'amia', 'mind', 'bads']\n",
      "['gamb', 'amie', 'mind', 'beds']\n",
      "['gamb', 'aria', 'mind', 'bade']\n",
      "['gamb', 'aria', 'mind', 'bads']\n",
      "['game', 'acid', 'mind', 'eddo']\n",
      "['game', 'acid', 'mind', 'eddy']\n",
      "['game', 'agin', 'mind', 'ends']\n",
      "['game', 'akin', 'mind', 'ends']\n",
      "['game', 'amid', 'mind', 'eddo']\n",
      "['game', 'amid', 'mind', 'eddy']\n",
      "['game', 'amin', 'mind', 'ends']\n",
      "['game', 'anil', 'mind', 'elds']\n",
      "['game', 'arid', 'mind', 'eddo']\n",
      "['game', 'arid', 'mind', 'eddy']\n",
      "['game', 'aril', 'mind', 'elds']\n",
      "['game', 'avid', 'mind', 'eddo']\n",
      "['game', 'avid', 'mind', 'eddy']\n",
      "['game', 'axil', 'mind', 'elds']\n",
      "['game', 'ayin', 'mind', 'ends']\n",
      "['gamp', 'agio', 'mind', 'pods']\n",
      "['gamp', 'amia', 'mind', 'pads']\n",
      "['gamp', 'amie', 'mind', 'peds']\n",
      "['gamp', 'aria', 'mind', 'pads']\n",
      "['gams', 'agio', 'mind', 'soda']\n",
      "['gams', 'agio', 'mind', 'sods']\n",
      "['gams', 'amia', 'mind', 'sade']\n",
      "['gams', 'amia', 'mind', 'sadi']\n",
      "['gams', 'aria', 'mind', 'sade']\n",
      "['gams', 'aria', 'mind', 'sadi']\n",
      "['gamy', 'agio', 'mind', 'yodh']\n",
      "['gamy', 'agio', 'mind', 'yods']\n",
      "['germ', 'etui', 'ruin', 'mind']\n",
      "['gimp', 'ilia', 'mind', 'pads']\n",
      "['gimp', 'inia', 'mind', 'pads']\n",
      "['gimp', 'ixia', 'mind', 'pads']\n",
      "['glim', 'loci', 'icon', 'mind']\n",
      "['gram', 'ragi', 'agin', 'mind']\n",
      "['gram', 'ragi', 'agon', 'mind']\n",
      "['gram', 'raki', 'akin', 'mind']\n",
      "['gram', 'rami', 'amen', 'mind']\n",
      "['gram', 'rami', 'amin', 'mind']\n",
      "['gram', 'rani', 'anon', 'mind']\n",
      "['grim', 'raki', 'ikon', 'mind']\n",
      "['hame', 'acid', 'mind', 'eddo']\n",
      "['hame', 'acid', 'mind', 'eddy']\n",
      "['hame', 'agin', 'mind', 'ends']\n",
      "['hame', 'akin', 'mind', 'ends']\n",
      "['hame', 'amid', 'mind', 'eddo']\n",
      "['hame', 'amid', 'mind', 'eddy']\n",
      "['hame', 'amin', 'mind', 'ends']\n",
      "['hame', 'anil', 'mind', 'elds']\n",
      "['hame', 'arid', 'mind', 'eddo']\n",
      "['hame', 'arid', 'mind', 'eddy']\n",
      "['hame', 'aril', 'mind', 'elds']\n",
      "['hame', 'avid', 'mind', 'eddo']\n",
      "['hame', 'avid', 'mind', 'eddy']\n",
      "['hame', 'axil', 'mind', 'elds']\n",
      "['hame', 'ayin', 'mind', 'ends']\n",
      "['hams', 'agio', 'mind', 'soda']\n",
      "['hams', 'agio', 'mind', 'sods']\n",
      "['hams', 'amia', 'mind', 'sade']\n",
      "['hams', 'amia', 'mind', 'sadi']\n",
      "['hams', 'aria', 'mind', 'sade']\n",
      "['hams', 'aria', 'mind', 'sadi']\n",
      "['heme', 'evil', 'mind', 'elds']\n",
      "['herm', 'etui', 'ruin', 'mind']\n",
      "['home', 'oxid', 'mind', 'eddo']\n",
      "['home', 'oxid', 'mind', 'eddy']\n",
      "['homo', 'oxid', 'mind', 'odds']\n",
      "['homy', 'olio', 'mind', 'yodh']\n",
      "['homy', 'olio', 'mind', 'yods']\n",
      "['iamb', 'agio', 'mind', 'bode']\n",
      "['iamb', 'agio', 'mind', 'bods']\n",
      "['iamb', 'agio', 'mind', 'body']\n",
      "['iamb', 'amia', 'mind', 'bade']\n",
      "['iamb', 'amia', 'mind', 'bads']\n",
      "['iamb', 'amie', 'mind', 'beds']\n",
      "['iamb', 'aria', 'mind', 'bade']\n",
      "['iamb', 'aria', 'mind', 'bads']\n",
      "['idem', 'deli', 'elan', 'mind']\n",
      "['imam', 'magi', 'agin', 'mind']\n",
      "['imam', 'magi', 'agon', 'mind']\n",
      "['imam', 'maxi', 'axon', 'mind']\n",
      "['imam', 'mini', 'anon', 'mind']\n",
      "['imam', 'momi', 'amen', 'mind']\n",
      "['imam', 'momi', 'amin', 'mind']\n",
      "['item', 'tali', 'elan', 'mind']\n",
      "['jamb', 'agio', 'mind', 'bode']\n",
      "['jamb', 'agio', 'mind', 'bods']\n",
      "['jamb', 'agio', 'mind', 'body']\n",
      "['jamb', 'amia', 'mind', 'bade']\n",
      "['jamb', 'amia', 'mind', 'bads']\n",
      "['jamb', 'amie', 'mind', 'beds']\n",
      "['jamb', 'aria', 'mind', 'bade']\n",
      "['jamb', 'aria', 'mind', 'bads']\n",
      "['jams', 'agio', 'mind', 'soda']\n",
      "['jams', 'agio', 'mind', 'sods']\n",
      "['jams', 'amia', 'mind', 'sade']\n",
      "['jams', 'amia', 'mind', 'sadi']\n",
      "['jams', 'aria', 'mind', 'sade']\n",
      "['jams', 'aria', 'mind', 'sadi']\n",
      "['jimp', 'ilia', 'mind', 'pads']\n",
      "['jimp', 'inia', 'mind', 'pads']\n",
      "['jimp', 'ixia', 'mind', 'pads']\n",
      "['kame', 'acid', 'mind', 'eddo']\n",
      "['kame', 'acid', 'mind', 'eddy']\n",
      "['kame', 'agin', 'mind', 'ends']\n",
      "['kame', 'akin', 'mind', 'ends']\n",
      "['kame', 'amid', 'mind', 'eddo']\n",
      "['kame', 'amid', 'mind', 'eddy']\n",
      "['kame', 'amin', 'mind', 'ends']\n",
      "['kame', 'anil', 'mind', 'elds']\n",
      "['kame', 'arid', 'mind', 'eddo']\n",
      "['kame', 'arid', 'mind', 'eddy']\n",
      "['kame', 'aril', 'mind', 'elds']\n",
      "['kame', 'avid', 'mind', 'eddo']\n",
      "['kame', 'avid', 'mind', 'eddy']\n",
      "['kame', 'axil', 'mind', 'elds']\n",
      "['kame', 'ayin', 'mind', 'ends']\n",
      "['lama', 'acid', 'mind', 'adds']\n",
      "['lama', 'agin', 'mind', 'ands']\n",
      "['lama', 'akin', 'mind', 'ands']\n",
      "['lama', 'amid', 'mind', 'adds']\n",
      "['lama', 'amin', 'mind', 'ands']\n",
      "['lama', 'arid', 'mind', 'adds']\n",
      "['lama', 'avid', 'mind', 'adds']\n",
      "['lama', 'ayin', 'mind', 'ands']\n",
      "['lamb', 'agio', 'mind', 'bode']\n",
      "['lamb', 'agio', 'mind', 'bods']\n",
      "['lamb', 'agio', 'mind', 'body']\n",
      "['lamb', 'amia', 'mind', 'bade']\n",
      "['lamb', 'amia', 'mind', 'bads']\n",
      "['lamb', 'amie', 'mind', 'beds']\n",
      "['lamb', 'aria', 'mind', 'bade']\n",
      "['lamb', 'aria', 'mind', 'bads']\n",
      "['lame', 'acid', 'mind', 'eddo']\n",
      "['lame', 'acid', 'mind', 'eddy']\n",
      "['lame', 'agin', 'mind', 'ends']\n",
      "['lame', 'akin', 'mind', 'ends']\n",
      "['lame', 'amid', 'mind', 'eddo']\n",
      "['lame', 'amid', 'mind', 'eddy']\n",
      "['lame', 'amin', 'mind', 'ends']\n",
      "['lame', 'anil', 'mind', 'elds']\n",
      "['lame', 'arid', 'mind', 'eddo']\n",
      "['lame', 'arid', 'mind', 'eddy']\n",
      "['lame', 'aril', 'mind', 'elds']\n",
      "['lame', 'avid', 'mind', 'eddo']\n",
      "['lame', 'avid', 'mind', 'eddy']\n",
      "['lame', 'axil', 'mind', 'elds']\n",
      "['lame', 'ayin', 'mind', 'ends']\n",
      "['lamp', 'agio', 'mind', 'pods']\n",
      "['lamp', 'amia', 'mind', 'pads']\n",
      "['lamp', 'amie', 'mind', 'peds']\n",
      "['lamp', 'aria', 'mind', 'pads']\n",
      "['lams', 'agio', 'mind', 'soda']\n",
      "['lams', 'agio', 'mind', 'sods']\n",
      "['lams', 'amia', 'mind', 'sade']\n",
      "['lams', 'amia', 'mind', 'sadi']\n",
      "['lams', 'aria', 'mind', 'sade']\n",
      "['lams', 'aria', 'mind', 'sadi']\n",
      "['lima', 'imid', 'mind', 'adds']\n",
      "['limb', 'ilia', 'mind', 'bade']\n",
      "['limb', 'ilia', 'mind', 'bads']\n",
      "['limb', 'inia', 'mind', 'bade']\n",
      "['limb', 'inia', 'mind', 'bads']\n",
      "['limb', 'ixia', 'mind', 'bade']\n",
      "['limb', 'ixia', 'mind', 'bads']\n",
      "['lime', 'imid', 'mind', 'eddo']\n",
      "['lime', 'imid', 'mind', 'eddy']\n",
      "['limo', 'imid', 'mind', 'odds']\n",
      "['limp', 'ilia', 'mind', 'pads']\n",
      "['limp', 'inia', 'mind', 'pads']\n",
      "['limp', 'ixia', 'mind', 'pads']\n",
      "['maim', 'abri', 'iron', 'mind']\n",
      "['maim', 'asci', 'icon', 'mind']\n",
      "['mama', 'acid', 'mind', 'adds']\n",
      "['mama', 'agin', 'mind', 'ands']\n",
      "['mama', 'akin', 'mind', 'ands']\n",
      "['mama', 'amid', 'mind', 'adds']\n",
      "['mama', 'amin', 'mind', 'ands']\n",
      "['mama', 'arid', 'mind', 'adds']\n",
      "['mama', 'avid', 'mind', 'adds']\n",
      "['mama', 'ayin', 'mind', 'ands']\n",
      "['memo', 'emir', 'mind', 'ordo']\n",
      "['memo', 'evil', 'mind', 'olds']\n",
      "['mime', 'imid', 'mind', 'eddo']\n",
      "['mime', 'imid', 'mind', 'eddy']\n",
      "['mind', 'idea', 'neap', 'daps']\n",
      "['mind', 'idea', 'near', 'darb']\n",
      "['mind', 'idea', 'near', 'dare']\n",
      "['mind', 'idea', 'near', 'dark']\n",
      "['mind', 'idea', 'near', 'darn']\n",
      "['mind', 'idea', 'near', 'dart']\n",
      "['mind', 'idea', 'neat', 'data']\n",
      "['mind', 'idea', 'neat', 'date']\n",
      "['mind', 'idea', 'neat', 'dato']\n",
      "['mind', 'idea', 'nebs', 'dash']\n",
      "['mind', 'idea', 'neck', 'daks']\n",
      "['mind', 'idea', 'need', 'dada']\n",
      "['mind', 'idea', 'need', 'dado']\n",
      "['mind', 'idea', 'need', 'dads']\n",
      "['mind', 'idea', 'neem', 'dame']\n",
      "['mind', 'idea', 'neem', 'damn']\n",
      "['mind', 'idea', 'neem', 'damp']\n",
      "['mind', 'idea', 'neem', 'dams']\n",
      "['mind', 'idea', 'neep', 'daps']\n",
      "['mind', 'idea', 'neif', 'daff']\n",
      "['mind', 'idea', 'neif', 'daft']\n",
      "['mind', 'idea', 'neon', 'dang']\n",
      "['mind', 'idea', 'neon', 'dank']\n",
      "['mind', 'idea', 'ness', 'dash']\n",
      "['mind', 'idea', 'nest', 'data']\n",
      "['mind', 'idea', 'nest', 'date']\n",
      "['mind', 'idea', 'nest', 'dato']\n",
      "['mind', 'idea', 'nets', 'dash']\n",
      "['mind', 'idea', 'nett', 'data']\n",
      "['mind', 'idea', 'nett', 'date']\n",
      "['mind', 'idea', 'nett', 'dato']\n",
      "['mind', 'idea', 'neum', 'dame']\n",
      "['mind', 'idea', 'neum', 'damn']\n",
      "['mind', 'idea', 'neum', 'damp']\n",
      "['mind', 'idea', 'neum', 'dams']\n",
      "['mind', 'idea', 'nevi', 'dais']\n",
      "['mind', 'idea', 'news', 'dash']\n",
      "['mind', 'idea', 'newt', 'data']\n",
      "['mind', 'idea', 'newt', 'date']\n",
      "['mind', 'idea', 'newt', 'dato']\n",
      "['mind', 'idea', 'next', 'data']\n",
      "['mind', 'idea', 'next', 'date']\n",
      "['mind', 'idea', 'next', 'dato']\n",
      "['mind', 'ilea', 'neap', 'daps']\n",
      "['mind', 'ilea', 'near', 'darb']\n",
      "['mind', 'ilea', 'near', 'dare']\n",
      "['mind', 'ilea', 'near', 'dark']\n",
      "['mind', 'ilea', 'near', 'darn']\n",
      "['mind', 'ilea', 'near', 'dart']\n",
      "['mind', 'ilea', 'neat', 'data']\n",
      "['mind', 'ilea', 'neat', 'date']\n",
      "['mind', 'ilea', 'neat', 'dato']\n",
      "['mind', 'ilea', 'nebs', 'dash']\n",
      "['mind', 'ilea', 'neck', 'daks']\n",
      "['mind', 'ilea', 'need', 'dada']\n",
      "['mind', 'ilea', 'need', 'dado']\n",
      "['mind', 'ilea', 'need', 'dads']\n",
      "['mind', 'ilea', 'neem', 'dame']\n",
      "['mind', 'ilea', 'neem', 'damn']\n",
      "['mind', 'ilea', 'neem', 'damp']\n",
      "['mind', 'ilea', 'neem', 'dams']\n",
      "['mind', 'ilea', 'neep', 'daps']\n",
      "['mind', 'ilea', 'neif', 'daff']\n",
      "['mind', 'ilea', 'neif', 'daft']\n",
      "['mind', 'ilea', 'neon', 'dang']\n",
      "['mind', 'ilea', 'neon', 'dank']\n",
      "['mind', 'ilea', 'ness', 'dash']\n",
      "['mind', 'ilea', 'nest', 'data']\n",
      "['mind', 'ilea', 'nest', 'date']\n",
      "['mind', 'ilea', 'nest', 'dato']\n",
      "['mind', 'ilea', 'nets', 'dash']\n",
      "['mind', 'ilea', 'nett', 'data']\n",
      "['mind', 'ilea', 'nett', 'date']\n",
      "['mind', 'ilea', 'nett', 'dato']\n",
      "['mind', 'ilea', 'neum', 'dame']\n",
      "['mind', 'ilea', 'neum', 'damn']\n",
      "['mind', 'ilea', 'neum', 'damp']\n",
      "['mind', 'ilea', 'neum', 'dams']\n",
      "['mind', 'ilea', 'nevi', 'dais']\n",
      "['mind', 'ilea', 'news', 'dash']\n",
      "['mind', 'ilea', 'newt', 'data']\n",
      "['mind', 'ilea', 'newt', 'date']\n",
      "['mind', 'ilea', 'newt', 'dato']\n",
      "['mind', 'ilea', 'next', 'data']\n",
      "['mind', 'ilea', 'next', 'date']\n",
      "['mind', 'ilea', 'next', 'dato']\n",
      "['mind', 'ilia', 'nibs', 'dash']\n",
      "['mind', 'ilia', 'nick', 'daks']\n",
      "['mind', 'ilia', 'nidi', 'dais']\n",
      "['mind', 'ilia', 'nigh', 'dahs']\n",
      "['mind', 'ilia', 'nill', 'dale']\n",
      "['mind', 'ilia', 'nils', 'dash']\n",
      "['mind', 'ilia', 'nims', 'dash']\n",
      "['mind', 'ilia', 'nips', 'dash']\n",
      "['mind', 'ilia', 'nisi', 'dais']\n",
      "['mind', 'ilia', 'nits', 'dash']\n",
      "['mind', 'ilia', 'nixy', 'days']\n",
      "['mind', 'inia', 'nibs', 'dash']\n",
      "['mind', 'inia', 'nick', 'daks']\n",
      "['mind', 'inia', 'nidi', 'dais']\n",
      "['mind', 'inia', 'nigh', 'dahs']\n",
      "['mind', 'inia', 'nill', 'dale']\n",
      "['mind', 'inia', 'nils', 'dash']\n",
      "['mind', 'inia', 'nims', 'dash']\n",
      "['mind', 'inia', 'nips', 'dash']\n",
      "['mind', 'inia', 'nisi', 'dais']\n",
      "['mind', 'inia', 'nits', 'dash']\n",
      "['mind', 'inia', 'nixy', 'days']\n",
      "['mind', 'ixia', 'nibs', 'dash']\n",
      "['mind', 'ixia', 'nick', 'daks']\n",
      "['mind', 'ixia', 'nidi', 'dais']\n",
      "['mind', 'ixia', 'nigh', 'dahs']\n",
      "['mind', 'ixia', 'nill', 'dale']\n",
      "['mind', 'ixia', 'nils', 'dash']\n",
      "['mind', 'ixia', 'nims', 'dash']\n",
      "['mind', 'ixia', 'nips', 'dash']\n",
      "['mind', 'ixia', 'nisi', 'dais']\n",
      "['mind', 'ixia', 'nits', 'dash']\n",
      "['mind', 'ixia', 'nixy', 'days']\n",
      "['mind', 'izar', 'name', 'dree']\n",
      "['mind', 'izar', 'name', 'dreg']\n",
      "['mind', 'izar', 'name', 'drek']\n",
      "['mind', 'izar', 'name', 'drew']\n",
      "['mind', 'izar', 'nana', 'drab']\n",
      "['mind', 'izar', 'nana', 'drag']\n",
      "['mind', 'izar', 'nana', 'dram']\n",
      "['mind', 'izar', 'nana', 'drat']\n",
      "['mind', 'izar', 'nana', 'draw']\n",
      "['mind', 'izar', 'nana', 'dray']\n",
      "['mind', 'izar', 'naoi', 'drib']\n",
      "['mind', 'izar', 'naoi', 'drip']\n",
      "['mind', 'izar', 'nape', 'dree']\n",
      "['mind', 'izar', 'nape', 'dreg']\n",
      "['mind', 'izar', 'nape', 'drek']\n",
      "['mind', 'izar', 'nape', 'drew']\n",
      "['mind', 'izar', 'nary', 'drys']\n",
      "['mind', 'izar', 'nave', 'dree']\n",
      "['mind', 'izar', 'nave', 'dreg']\n",
      "['mind', 'izar', 'nave', 'drek']\n",
      "['mind', 'izar', 'nave', 'drew']\n",
      "['mind', 'izar', 'navy', 'drys']\n",
      "['mind', 'izar', 'nazi', 'drib']\n",
      "['mind', 'izar', 'nazi', 'drip']\n",
      "['mome', 'oxid', 'mind', 'eddo']\n",
      "['mome', 'oxid', 'mind', 'eddy']\n",
      "['moms', 'obia', 'mind', 'sade']\n",
      "['moms', 'obia', 'mind', 'sadi']\n",
      "['moms', 'ohia', 'mind', 'sade']\n",
      "['moms', 'ohia', 'mind', 'sadi']\n",
      "['moms', 'olio', 'mind', 'soda']\n",
      "['moms', 'olio', 'mind', 'sods']\n",
      "['mumm', 'unai', 'main', 'mind']\n",
      "['mumm', 'unai', 'maun', 'mind']\n",
      "['mumm', 'unai', 'mawn', 'mind']\n",
      "['name', 'acid', 'mind', 'eddo']\n",
      "['name', 'acid', 'mind', 'eddy']\n",
      "['name', 'agin', 'mind', 'ends']\n",
      "['name', 'akin', 'mind', 'ends']\n",
      "['name', 'amid', 'mind', 'eddo']\n",
      "['name', 'amid', 'mind', 'eddy']\n",
      "['name', 'amin', 'mind', 'ends']\n",
      "['name', 'anil', 'mind', 'elds']\n",
      "['name', 'arid', 'mind', 'eddo']\n",
      "['name', 'arid', 'mind', 'eddy']\n",
      "['name', 'aril', 'mind', 'elds']\n",
      "['name', 'avid', 'mind', 'eddo']\n",
      "['name', 'avid', 'mind', 'eddy']\n",
      "['name', 'axil', 'mind', 'elds']\n",
      "['name', 'ayin', 'mind', 'ends']\n",
      "['nims', 'ilia', 'mind', 'sade']\n",
      "['nims', 'ilia', 'mind', 'sadi']\n",
      "['nims', 'inia', 'mind', 'sade']\n",
      "['nims', 'inia', 'mind', 'sadi']\n",
      "['nims', 'ixia', 'mind', 'sade']\n",
      "['nims', 'ixia', 'mind', 'sadi']\n",
      "['noma', 'oxid', 'mind', 'adds']\n",
      "['nome', 'oxid', 'mind', 'eddo']\n",
      "['nome', 'oxid', 'mind', 'eddy']\n",
      "['noms', 'obia', 'mind', 'sade']\n",
      "['noms', 'obia', 'mind', 'sadi']\n",
      "['noms', 'ohia', 'mind', 'sade']\n",
      "['noms', 'ohia', 'mind', 'sadi']\n",
      "['noms', 'olio', 'mind', 'soda']\n",
      "['noms', 'olio', 'mind', 'sods']\n",
      "['pams', 'agio', 'mind', 'soda']\n",
      "['pams', 'agio', 'mind', 'sods']\n",
      "['pams', 'amia', 'mind', 'sade']\n",
      "['pams', 'amia', 'mind', 'sadi']\n",
      "['pams', 'aria', 'mind', 'sade']\n",
      "['pams', 'aria', 'mind', 'sadi']\n",
      "['perm', 'etui', 'ruin', 'mind']\n",
      "['pima', 'imid', 'mind', 'adds']\n",
      "['pimp', 'ilia', 'mind', 'pads']\n",
      "['pimp', 'inia', 'mind', 'pads']\n",
      "['pimp', 'ixia', 'mind', 'pads']\n",
      "['pome', 'oxid', 'mind', 'eddo']\n",
      "['pome', 'oxid', 'mind', 'eddy']\n",
      "['pomp', 'obia', 'mind', 'pads']\n",
      "['pomp', 'ohia', 'mind', 'pads']\n",
      "['pomp', 'olio', 'mind', 'pods']\n",
      "['pram', 'ragi', 'agin', 'mind']\n",
      "['pram', 'ragi', 'agon', 'mind']\n",
      "['pram', 'raki', 'akin', 'mind']\n",
      "['pram', 'rami', 'amen', 'mind']\n",
      "['pram', 'rami', 'amin', 'mind']\n",
      "['pram', 'rani', 'anon', 'mind']\n",
      "['prim', 'raki', 'ikon', 'mind']\n",
      "['prom', 'rami', 'omen', 'mind']\n",
      "['ramp', 'agio', 'mind', 'pods']\n",
      "['ramp', 'amia', 'mind', 'pads']\n",
      "['ramp', 'amie', 'mind', 'peds']\n",
      "['ramp', 'aria', 'mind', 'pads']\n",
      "['rams', 'agio', 'mind', 'soda']\n",
      "['rams', 'agio', 'mind', 'sods']\n",
      "['rams', 'amia', 'mind', 'sade']\n",
      "['rams', 'amia', 'mind', 'sadi']\n",
      "['rams', 'aria', 'mind', 'sade']\n",
      "['rams', 'aria', 'mind', 'sadi']\n",
      "['rime', 'imid', 'mind', 'eddo']\n",
      "['rime', 'imid', 'mind', 'eddy']\n",
      "['rims', 'ilia', 'mind', 'sade']\n",
      "['rims', 'ilia', 'mind', 'sadi']\n",
      "['rims', 'inia', 'mind', 'sade']\n",
      "['rims', 'inia', 'mind', 'sadi']\n",
      "['rims', 'ixia', 'mind', 'sade']\n",
      "['rims', 'ixia', 'mind', 'sadi']\n",
      "['romp', 'obia', 'mind', 'pads']\n",
      "['romp', 'ohia', 'mind', 'pads']\n",
      "['romp', 'olio', 'mind', 'pods']\n",
      "['same', 'acid', 'mind', 'eddo']\n",
      "['same', 'acid', 'mind', 'eddy']\n",
      "['same', 'agin', 'mind', 'ends']\n",
      "['same', 'akin', 'mind', 'ends']\n",
      "['same', 'amid', 'mind', 'eddo']\n",
      "['same', 'amid', 'mind', 'eddy']\n",
      "['same', 'amin', 'mind', 'ends']\n",
      "['same', 'anil', 'mind', 'elds']\n",
      "['same', 'arid', 'mind', 'eddo']\n",
      "['same', 'arid', 'mind', 'eddy']\n",
      "['same', 'aril', 'mind', 'elds']\n",
      "['same', 'avid', 'mind', 'eddo']\n",
      "['same', 'avid', 'mind', 'eddy']\n",
      "['same', 'axil', 'mind', 'elds']\n",
      "['same', 'ayin', 'mind', 'ends']\n",
      "['samp', 'agio', 'mind', 'pods']\n",
      "['samp', 'amia', 'mind', 'pads']\n",
      "['samp', 'amie', 'mind', 'peds']\n",
      "['samp', 'aria', 'mind', 'pads']\n",
      "['scam', 'coni', 'anon', 'mind']\n",
      "['seme', 'evil', 'mind', 'elds']\n",
      "['sham', 'hili', 'alan', 'mind']\n",
      "['shmo', 'hail', 'mind', 'olds']\n",
      "['shmo', 'hair', 'mind', 'ordo']\n",
      "['shmo', 'heil', 'mind', 'olds']\n",
      "['shmo', 'heir', 'mind', 'ordo']\n",
      "['sima', 'imid', 'mind', 'adds']\n",
      "['simp', 'ilia', 'mind', 'pads']\n",
      "['simp', 'inia', 'mind', 'pads']\n",
      "['simp', 'ixia', 'mind', 'pads']\n",
      "['sims', 'ilia', 'mind', 'sade']\n",
      "['sims', 'ilia', 'mind', 'sadi']\n",
      "['sims', 'inia', 'mind', 'sade']\n",
      "['sims', 'inia', 'mind', 'sadi']\n",
      "['sims', 'ixia', 'mind', 'sade']\n",
      "['sims', 'ixia', 'mind', 'sadi']\n",
      "['skim', 'kaki', 'ikon', 'mind']\n",
      "['slim', 'loci', 'icon', 'mind']\n",
      "['soma', 'oxid', 'mind', 'adds']\n",
      "['some', 'oxid', 'mind', 'eddo']\n",
      "['some', 'oxid', 'mind', 'eddy']\n",
      "['stem', 'tali', 'elan', 'mind']\n",
      "['stum', 'tali', 'ulan', 'mind']\n",
      "['stum', 'tipi', 'upon', 'mind']\n",
      "['stum', 'topi', 'upon', 'mind']\n",
      "['tame', 'acid', 'mind', 'eddo']\n",
      "['tame', 'acid', 'mind', 'eddy']\n",
      "['tame', 'agin', 'mind', 'ends']\n",
      "['tame', 'akin', 'mind', 'ends']\n",
      "['tame', 'amid', 'mind', 'eddo']\n",
      "['tame', 'amid', 'mind', 'eddy']\n",
      "['tame', 'amin', 'mind', 'ends']\n",
      "['tame', 'anil', 'mind', 'elds']\n",
      "['tame', 'arid', 'mind', 'eddo']\n",
      "['tame', 'arid', 'mind', 'eddy']\n",
      "['tame', 'aril', 'mind', 'elds']\n",
      "['tame', 'avid', 'mind', 'eddo']\n",
      "['tame', 'avid', 'mind', 'eddy']\n",
      "['tame', 'axil', 'mind', 'elds']\n",
      "['tame', 'ayin', 'mind', 'ends']\n",
      "['tamp', 'agio', 'mind', 'pods']\n",
      "['tamp', 'amia', 'mind', 'pads']\n",
      "['tamp', 'amie', 'mind', 'peds']\n",
      "['tamp', 'aria', 'mind', 'pads']\n",
      "['tams', 'agio', 'mind', 'soda']\n",
      "['tams', 'agio', 'mind', 'sods']\n",
      "['tams', 'amia', 'mind', 'sade']\n",
      "['tams', 'amia', 'mind', 'sadi']\n",
      "['tams', 'aria', 'mind', 'sade']\n",
      "['tams', 'aria', 'mind', 'sadi']\n",
      "['term', 'etui', 'ruin', 'mind']\n",
      "['them', 'hili', 'elan', 'mind']\n",
      "['time', 'imid', 'mind', 'eddo']\n",
      "['time', 'imid', 'mind', 'eddy']\n",
      "['tomb', 'obia', 'mind', 'bade']\n",
      "['tomb', 'obia', 'mind', 'bads']\n",
      "['tomb', 'ohia', 'mind', 'bade']\n",
      "['tomb', 'ohia', 'mind', 'bads']\n",
      "['tomb', 'olio', 'mind', 'bode']\n",
      "['tomb', 'olio', 'mind', 'bods']\n",
      "['tomb', 'olio', 'mind', 'body']\n",
      "['tome', 'oxid', 'mind', 'eddo']\n",
      "['tome', 'oxid', 'mind', 'eddy']\n",
      "['toms', 'obia', 'mind', 'sade']\n",
      "['toms', 'obia', 'mind', 'sadi']\n",
      "['toms', 'ohia', 'mind', 'sade']\n",
      "['toms', 'ohia', 'mind', 'sadi']\n",
      "['toms', 'olio', 'mind', 'soda']\n",
      "['toms', 'olio', 'mind', 'sods']\n",
      "['tram', 'ragi', 'agin', 'mind']\n",
      "['tram', 'ragi', 'agon', 'mind']\n",
      "['tram', 'raki', 'akin', 'mind']\n",
      "['tram', 'rami', 'amen', 'mind']\n",
      "['tram', 'rami', 'amin', 'mind']\n",
      "['tram', 'rani', 'anon', 'mind']\n",
      "['trim', 'raki', 'ikon', 'mind']\n",
      "['vamp', 'agio', 'mind', 'pods']\n",
      "['vamp', 'amia', 'mind', 'pads']\n",
      "['vamp', 'amie', 'mind', 'peds']\n",
      "['vamp', 'aria', 'mind', 'pads']\n",
      "['vims', 'ilia', 'mind', 'sade']\n",
      "['vims', 'ilia', 'mind', 'sadi']\n",
      "['vims', 'inia', 'mind', 'sade']\n",
      "['vims', 'inia', 'mind', 'sadi']\n",
      "['vims', 'ixia', 'mind', 'sade']\n",
      "['vims', 'ixia', 'mind', 'sadi']\n",
      "['wame', 'acid', 'mind', 'eddo']\n",
      "['wame', 'acid', 'mind', 'eddy']\n",
      "['wame', 'agin', 'mind', 'ends']\n",
      "['wame', 'akin', 'mind', 'ends']\n",
      "['wame', 'amid', 'mind', 'eddo']\n",
      "['wame', 'amid', 'mind', 'eddy']\n",
      "['wame', 'amin', 'mind', 'ends']\n",
      "['wame', 'anil', 'mind', 'elds']\n",
      "['wame', 'arid', 'mind', 'eddo']\n",
      "['wame', 'arid', 'mind', 'eddy']\n",
      "['wame', 'aril', 'mind', 'elds']\n",
      "['wame', 'avid', 'mind', 'eddo']\n",
      "['wame', 'avid', 'mind', 'eddy']\n",
      "['wame', 'axil', 'mind', 'elds']\n",
      "['wame', 'ayin', 'mind', 'ends']\n",
      "['wham', 'hili', 'alan', 'mind']\n",
      "['womb', 'obia', 'mind', 'bade']\n",
      "['womb', 'obia', 'mind', 'bads']\n",
      "['womb', 'ohia', 'mind', 'bade']\n",
      "['womb', 'ohia', 'mind', 'bads']\n",
      "['womb', 'olio', 'mind', 'bode']\n",
      "['womb', 'olio', 'mind', 'bods']\n",
      "['womb', 'olio', 'mind', 'body']\n",
      "['yams', 'agio', 'mind', 'soda']\n",
      "['yams', 'agio', 'mind', 'sods']\n",
      "['yams', 'amia', 'mind', 'sade']\n",
      "['yams', 'amia', 'mind', 'sadi']\n",
      "['yams', 'aria', 'mind', 'sade']\n",
      "['yams', 'aria', 'mind', 'sadi']\n"
     ]
    }
   ],
   "source": [
    "find_word_in_word_squares(four_word_square, \"mind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While it's not very difficult to expand the non-recursive version of the 4x4 puzzle generator above, it's easy to see how a generalizable version would be preferable, even if there are few possible word squares past a given size.  However, the recursive versions are slower, and as the search space grows quadratically, large puzzles can be very slow to solve.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-recursive version of 5 x 5 puzzle\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "five_wl = make_n_letter_wordlist('words.txt', 5)\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "five_d = make_n_minus1_dict(five_wl, 5)\n",
    "\n",
    "five_ws = []\n",
    "\n",
    "for w in five_wl:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in five_d:\n",
    "        for cand1 in five_d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in five_d:\n",
    "                for cand2 in five_d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in five_d:\n",
    "                        for cand3 in five_d[check3]:\n",
    "                            check4 = test[4] + cand1[4] + cand2[4] + cand3[4]\n",
    "                            if check4 in five_d:\n",
    "                                for cand4 in five_d[check4]:\n",
    "                                    five_ws.append([test, cand1, cand2, cand3, cand4])\n",
    "                            \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique word squares: 4,545,905.\n",
      "Total elapsed time for the non-recursive version of the 5x5 puzzle: 81.4978 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique word squares: {:,}.\".format(len(five_ws)))\n",
    "print(\"Total elapsed time for the non-recursive version of the 5x5 puzzle: {:.4f} seconds.\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCUPS\n",
      "CURET\n",
      "URSAE\n",
      "PEASE\n",
      "STEEK\n"
     ]
    }
   ],
   "source": [
    "print_random_ws(five_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-c8560aaaf6d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfive_word_square\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_word_square\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-210-9192a87e9608>\u001b[0m in \u001b[0;36mmake_word_square\u001b[1;34m(text, n)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mrecursive_ws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-210-9192a87e9608>\u001b[0m in \u001b[0;36mrecursive_ws\u001b[1;34m(d, words, ws, n)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mrecursive_ws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_word_square\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-210-9192a87e9608>\u001b[0m in \u001b[0;36mrecursive_ws\u001b[1;34m(d, words, ws, n)\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mrecursive_ws\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake_word_square\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-210-9192a87e9608>\u001b[0m in \u001b[0;36mrecursive_ws\u001b[1;34m(d, words, ws, n)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m# evaluate other possible candidates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                 \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[0mwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "five_word_square = make_word_square('words.txt', 5)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(five_word_square)))\n",
    "print(\"Total elapsed time for the recursive version of the 5x5 puzzle: {:.4f} seconds\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As the squares get bigger, the time required goes up immensely.  Therefore, I'm not going to run the cells (again), and will only include the code as markdown, together with the original output:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start = time.time()\n",
    "\n",
    "six_word_square = make_word_square('words.txt', 6)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(six_word_square)))\n",
    "print(\"Total elapsed time for the recursive version of the 6x6 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "\n",
    "Number of unique word squares: 677,648.\n",
    "Total elapsed time for the recursive version of the 6x6 puzzle: 1908.6381 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print_random_ws(six_word_square)\n",
    "\n",
    "\n",
    "RIBBER\n",
    "IDEATE\n",
    "BECKED\n",
    "BAKERY\n",
    "ETERNE\n",
    "REDYES\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "len(six_word_square)\n",
    "\n",
    "677648\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# non-recursive version of 6 x 6 puzzle\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "six_wl = make_n_letter_wordlist('words.txt', 6)\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "six_d = make_n_minus1_dict(six_wl, 6)\n",
    "\n",
    "six_ws = []\n",
    "\n",
    "for w in six_wl:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in six_d:\n",
    "        for cand1 in six_d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in six_d:\n",
    "                for cand2 in six_d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in six_d:\n",
    "                        for cand3 in six_d[check3]:\n",
    "                            check4 = test[4] + cand1[4] + cand2[4] + cand3[4]\n",
    "                            if check4 in six_d:\n",
    "                                for cand4 in six_d[check4]: \n",
    "                                    check5 = test[5] + cand1[5] + cand2[5] + cand3[5] + cand4[5]\n",
    "                                    if check5 in six_d:\n",
    "                                        for cand5 in six_d[check5]:\n",
    "                                            six_ws.append([test, cand1, cand2, cand3, cand4, cand5])\n",
    "                 \n",
    "                            \n",
    "end = time.time()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(six_ws)))\n",
    "print(\"Total elapsed time for the non-recursive version of the 6x6 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 677,648.\n",
    "Total elapsed time for the non-recursive version of the 6x6 puzzle: 552.1832 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# non-recursive version of 7 x 7 puzzle\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "sev_wl = make_n_letter_wordlist('words.txt', 7)\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "sev_d = make_n_minus1_dict(sev_wl, 7)\n",
    "\n",
    "sev_ws = []\n",
    "\n",
    "for w in sev_wl:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in sev_d:\n",
    "        for cand1 in sev_d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in sev_d:\n",
    "                for cand2 in sev_d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in sev_d:\n",
    "                        for cand3 in sev_d[check3]:\n",
    "                            check4 = test[4] + cand1[4] + cand2[4] + cand3[4]\n",
    "                            if check4 in sev_d:\n",
    "                                for cand4 in sev_d[check4]: \n",
    "                                    check5 = test[5] + cand1[5] + cand2[5] + cand3[5] + cand4[5]\n",
    "                                    if check5 in sev_d:\n",
    "                                        for cand5 in sev_d[check5]:\n",
    "                                            check6 = test[6] + cand1[6] + cand2[6] + cand3[6] + cand4[6] + cand5[6]\n",
    "                                            if check6 in sev_d:\n",
    "                                                for cand6 in sev_d[check6]:\n",
    "                                                    sev_ws.append([test, cand1, cand2, cand3, cand4, cand5, cand6])\n",
    "\n",
    "                            \n",
    "end = time.time()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(sev_ws)))\n",
    "print(\"Total elapsed time for the non-recursive version of the 7x7 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 7,142.\n",
    "Total elapsed time for the non-recursive version of the 7x7 puzzle: 3310.1383 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print_random_ws(sev_ws)\n",
    "\n",
    "CODGERS\n",
    "OVERLET\n",
    "DECEASE\n",
    "GREASER\n",
    "ELASTIN\n",
    "RESEIZE\n",
    "STERNER\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# non-recursive version of 8 x 8 puzzle\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "eight_wl = make_n_letter_wordlist('words.txt', 8)\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "eight_d = make_n_minus1_dict(eight_wl, 8)\n",
    "\n",
    "eight_ws = []\n",
    "\n",
    "for w in eight_wl:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in eight_d:\n",
    "        for cand1 in eight_d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in eight_d:\n",
    "                for cand2 in eight_d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in eight_d:\n",
    "                        for cand3 in eight_d[check3]:\n",
    "                            check4 = test[4] + cand1[4] + cand2[4] + cand3[4]\n",
    "                            if check4 in eight_d:\n",
    "                                for cand4 in eight_d[check4]: \n",
    "                                    check5 = test[5] + cand1[5] + cand2[5] + cand3[5] + cand4[5]\n",
    "                                    if check5 in eight_d:\n",
    "                                        for cand5 in eight_d[check5]:\n",
    "                                            check6 = test[6] + cand1[6] + cand2[6] + cand3[6] + cand4[6] + cand5[6]\n",
    "                                            if check6 in eight_d:\n",
    "                                                for cand6 in eight_d[check6]:\n",
    "                                                    check7 = test[7] + cand1[7] + cand2[7] + cand3[7] + cand4[7] + cand5[7] + cand6[7]\n",
    "                                                    if check7 in eight_d:\n",
    "                                                        for cand7 in eight_d[check7]:\n",
    "                                                            eight_ws.append([test, cand1, cand2, cand3, cand4, cand5, cand6, cand7])  \n",
    "                                                            \n",
    "end = time.time()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "len(eight_wl)\n",
    "26447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(eight_ws)))\n",
    "print(\"Total elapsed time for the non-recursive version of the 8x8 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 2.\n",
    "Total elapsed time for the non-recursive version of the 8x8 puzzle: 5545.3752 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for e in eight_ws:\n",
    "    for word in e:\n",
    "        print(word.upper())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "CARBORAS\n",
    "APERIENT\n",
    "RECALLER\n",
    "BRASSICA\n",
    "OILSEEDS\n",
    "RELIEVOS\n",
    "ANECDOTE\n",
    "STRASSES\n",
    "\n",
    "\n",
    "CRABWISE\n",
    "RATLINES\n",
    "ATLANTES\n",
    "BLASTEMA\n",
    "WINTERLY\n",
    "INTERTIE\n",
    "SEEMLIER\n",
    "ESSAYERS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# non-recursive version of 9 x 9 puzzle\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "nine_wl = make_n_letter_wordlist('words.txt', 9)\n",
    "\n",
    "# making dictionaries of first, first-second, and first-third letters\n",
    "\n",
    "nine_d = make_n_minus1_dict(nine_wl, 9)\n",
    "\n",
    "nine_ws = []\n",
    "\n",
    "for w in nine_wl:\n",
    "    test = w\n",
    "    check1 = test[1]\n",
    "    if check1 in nine_d:\n",
    "        for cand1 in nine_d[check1]:\n",
    "            check2 = test[2] + cand1[2]\n",
    "            if check2 in nine_d:\n",
    "                for cand2 in nine_d[check2]:\n",
    "                    check3 = test[3] + cand1[3] + cand2[3]\n",
    "                    if check3 in nine_d:\n",
    "                        for cand3 in nine_d[check3]:\n",
    "                            check4 = test[4] + cand1[4] + cand2[4] + cand3[4]\n",
    "                            if check4 in nine_d:\n",
    "                                for cand4 in nine_d[check4]: \n",
    "                                    check5 = test[5] + cand1[5] + cand2[5] + cand3[5] + cand4[5]\n",
    "                                    if check5 in nine_d:\n",
    "                                        for cand5 in nine_d[check5]:\n",
    "                                            check6 = test[6] + cand1[6] + cand2[6] + cand3[6] + cand4[6] + cand5[6]\n",
    "                                            if check6 in nine_d:\n",
    "                                                for cand6 in nine_d[check6]:\n",
    "                                                    check7 = test[7] + cand1[7] + cand2[7] + cand3[7] + cand4[7] + cand5[7] + cand6[7]\n",
    "                                                    if check7 in nine_d:\n",
    "                                                        for cand7 in nine_d[check7]:\n",
    "                                                            check8 = test[8] + cand1[8] + cand2[8] + cand3[8] + cand4[8] + cand5[8] + cand6[8] + cand7[8]\n",
    "                                                            if check8 in nine_d:\n",
    "                                                                for cand8 in nine_d[check7]:\n",
    "                                                                    nine_ws.append([test, cand1, cand2, cand3, cand4, cand5, cand6, cand7, cand8])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(nine_ws)))\n",
    "print(\"Total elapsed time for the non-recursive version of the 9x9 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "\n",
    "\n",
    "Number of unique word squares: 0.\n",
    "Total elapsed time for the non-recursive version of the 9x9 puzzle: 1250.1177 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start = time.time()\n",
    "\n",
    "ten_word_square = make_word_square('words.txt', 10)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(ten_word_square)))\n",
    "print(\"Total elapsed time for the recursive version of the 10x10 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 0.\n",
    "Total elapsed time for the recursive version of the 10x10 puzzle: 466.3639 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start = time.time()\n",
    "\n",
    "ele_word_square = make_word_square('words.txt', 11)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(ele_word_square)))\n",
    "print(\"Total elapsed time for the recursive version of the 11x11 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 0.\n",
    "Total elapsed time for the recursive version of the 11x11 puzzle: 83.8175 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "start = time.time()\n",
    "\n",
    "twl_word_square = make_word_square('words.txt', 12)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Number of unique word squares: {:,}.\".format(len(twl_word_square)))\n",
    "print(\"Total elapsed time for the recursive version of the 12x12 puzzle: {:.4f} seconds\".format(end - start))\n",
    "\n",
    "Number of unique word squares: 0.\n",
    "Total elapsed time for the recursive version of the 11x11 puzzle: 14.9461 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*On my other machine I have csv files with all of the word squares.  But for some of the squares the file size is huge.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__TO DO:__ develop solution that uses tries to see if it's any quicker.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
