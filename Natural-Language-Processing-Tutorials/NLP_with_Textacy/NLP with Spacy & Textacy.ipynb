{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing With SpaCy & Textacy\n",
    "![textacylogo](textacylogo1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Textacy\n",
    "+ Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, \n",
    "+ built on the high-performance Spacy library.\n",
    "+ Textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text.\n",
    "    ### Uses\n",
    "    - Text preprocessing\n",
    "    - Keyword in Context\n",
    "    - Topic modeling\n",
    "    - Information Extraction\n",
    "    - Keyterm extraction, \n",
    "    - Text and Readability statistics,\n",
    "    - Emotional valence analysis, \n",
    "    - Quotation attribution\n",
    "\n",
    "#### Installation\n",
    "+ pip install textacy\n",
    "+ conda install -c conda-forge textacy\n",
    "\n",
    "##### Downloading Dataset\n",
    "+ python -m textacy download \n",
    "\n",
    "### For Language Detection\n",
    "+ pip install textacy[lang]\n",
    "+ pip install cld2-cffi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing With Textacy\n",
    "+ textacy.preprocess_text()\n",
    "+ textacy.preprocess.\n",
    "    - Punctuation Lowercase\n",
    "    - Urls \n",
    "    - Phone numbers\n",
    "    - Currency\n",
    "    - Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\" The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for $50 although in Paris it will cost $30 dollars.\n",
    "Don’t document the problem, fix it.This is from https://twitter.com/codewisdom?lang=en. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for $50 although in Paris it will cost $30 dollars \\nDon t document the problem  fix it This is from https   twitter com codewisdom lang=en  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuation and Uppercase\n",
    "textacy.preprocess.remove_punct(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for $50 although in Paris it will cost $30 dollars.\\nDon’t document the problem, fix it.This is from TWITTER '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing urls\n",
    "textacy.preprocess.replace_urls(raw_text,replace_with='TWITTER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for USD50 although in Paris it will cost USD30 dollars.\\nDon’t document the problem, fix it.This is from https://twitter.com/codewisdom?lang=en. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing Currency Symbols\n",
    "textacy.preprocess.replace_currency_symbols(raw_text,replace_with='USD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the best programs are the ones written when the programmer is supposed to be working on something else mike bought the book for $50 although in paris it will cost $30 dollars don t document the problem fix it this is from url'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess All\n",
    "textacy.preprocess_text(raw_text,lowercase=True,no_punct=True,no_urls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the best programs, are the ones written when the programmer is supposed to be working on something else.mike bought the book for $50 although in paris it will cost $30 dollars.\\ndon’t document the problem, fix it.this is from https://twitter.com/codewisdom?lang=en.\\ndebuggers don\\'t remove bugs. they only show them in slow motion.\\n\"if at first you don’t succeed, call it version 1.0.\"\\nin theory, there is no difference between theory and practice. but, in practice, there is.\\n\"commenting your code is like cleaning your bathroom - you never want to do it, but it really does create a more pleasant experience for you and your guests.\" - ryan campbell\\nyour problem is another\\'s solution; your solution will be their problem.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Processing a Text on a File\n",
    "textacy.preprocess_text(open(\"sample.txt\").read(),lowercase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a Text or A Document\n",
    "+ textacy.Doc(your_text)\n",
    "+ textacy.io.read_text(your_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Doc\n",
    "# Requires Language Pkg Model\n",
    "docx_textacy = textacy.Doc(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(82 tokens; \"Textacy is a Python library for performing high...\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textacy.doc.Doc"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docx_textacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using spacy\n",
    "import spacy \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_spacy = nlp(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docx_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Both Are of the Type Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading A File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "file_textacy = textacy.Doc(open(\"example.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(471 tokens; \"The nativity of Jesus or birth of Jesus is desc...\")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2\n",
    "# Creates a generator\n",
    "# file_textacy2 = textacy.io.read_text('example.txt')\n",
    "file_textacy2 = textacy.io.read_text('example.txt',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file_textacy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(148 tokens; \"The nativity of Jesus or birth of Jesus is desc...\")\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'un'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-255992ae46a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_textacy2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdocx_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textacy/doc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, content, metadata, lang)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0municode_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpacyDoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_from_spacy_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textacy/doc.py\u001b[0m in \u001b[0;36m_init_from_text\u001b[0;34m(self, content, metadata, lang)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mlangstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mspacy_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_spacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlangstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             raise TypeError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/cachetools/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;32mpass\u001b[0m  \u001b[0;31m# key not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textacy/cache.py\u001b[0m in \u001b[0;36mload_spacy\u001b[0;34m(name, disable)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mdisable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading \"%s\" spaCy pipeline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exists'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'un'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "for text in file_textacy2:\n",
    "    docx_file = textacy.Doc(text)\n",
    "    print(docx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working With Multiple Text Documents\n",
    "+ textacy.io.read_text(text,lines=True)\n",
    "+ textacy.io.read_json(text,lines=True)\n",
    "+ textacy.io.csv.read_csv(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Analysis of Text\n",
    "+ Tokenization\n",
    "+ Ngrams\n",
    "+ Named Entities\n",
    "+ Key Terms & Text Rank\n",
    "+ Basic Counts/Frequency & Stats\n",
    "+ Bag of Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLP', 'ORG'), ('Spacy', 'GPE')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SpaCy Named Entities Recognition\n",
    "[ (entity.text,entity.label_) for entity in docx_spacy.ents ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NLP, Spacy]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Textacy Named Entity Extraction\n",
    "list(textacy.extract.named_entities(docx_textacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[library for performing,\n",
       " level natural language,\n",
       " natural language processing,\n",
       " performance Spacy library,\n",
       " With the basics,\n",
       " focuses on tasks,\n",
       " availability of tokenized,\n",
       " emotional valence analysis]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGrams with Textacy\n",
    "# NB SpaCy method would be to use noun Phrases\n",
    "# Tri Grams\n",
    "\n",
    "list(textacy.extract.ngrams(docx_textacy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info Extraction/Summary\n",
    "+ semistructured_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx = textacy.Doc(open(\"example1.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Points\n",
    "statements = textacy.extract.semistructured_statements(docx,\"Jerusalem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object semistructured_statements at 0x7f2403592db0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is about: \n",
      ":the third-holiest city, after Mecca and Medina.[26][27\n"
     ]
    }
   ],
   "source": [
    "# Prints Results\n",
    "print(\"This text is about: \")\n",
    "for statement in statements:\n",
    "    subject,verb,point = statement\n",
    "    print(f':{point}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Terms and Text Rank\n",
    "+ Textacy\n",
    "+ PyTextRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Keyterms for TextRank & Srank\n",
    "import textacy.keyterms\n",
    "# You can lemmatize it or normalize it for better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylemma = [(token.lemma_) for token in docx_textacy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textacy',\n",
       " 'be',\n",
       " 'a',\n",
       " 'python',\n",
       " 'library',\n",
       " 'for',\n",
       " 'perform',\n",
       " 'high',\n",
       " '-',\n",
       " 'level',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'nlp',\n",
       " ')',\n",
       " 'task',\n",
       " ',',\n",
       " 'build',\n",
       " 'on',\n",
       " 'the',\n",
       " 'high',\n",
       " '-',\n",
       " 'performance',\n",
       " 'spacy',\n",
       " 'library',\n",
       " '.',\n",
       " 'with',\n",
       " 'the',\n",
       " 'basic',\n",
       " '--',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'part',\n",
       " '-',\n",
       " 'of',\n",
       " '-',\n",
       " 'speech',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parse',\n",
       " '--',\n",
       " 'offload',\n",
       " 'to',\n",
       " 'another',\n",
       " 'library',\n",
       " ',',\n",
       " 'textacy',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'task',\n",
       " 'facilitate',\n",
       " 'by',\n",
       " 'the',\n",
       " 'availability',\n",
       " 'of',\n",
       " 'tokeniz',\n",
       " ',',\n",
       " 'pos',\n",
       " '-',\n",
       " 'tag',\n",
       " ',',\n",
       " 'and',\n",
       " 'parse',\n",
       " 'text',\n",
       " ':',\n",
       " 'keyterm',\n",
       " 'extraction',\n",
       " ',',\n",
       " 'readability',\n",
       " 'statistic',\n",
       " ',',\n",
       " 'emotional',\n",
       " 'valence',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'quotation',\n",
       " 'attribution',\n",
       " ',',\n",
       " 'and',\n",
       " 'more',\n",
       " '.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('library', 0.0760061484239787),\n",
       " ('task', 0.05354513480888809),\n",
       " ('high', 0.05240430760022597),\n",
       " ('quotation', 0.041359218381418185),\n",
       " ('textacy', 0.03905943157880573),\n",
       " ('analysis', 0.03883696307086067),\n",
       " ('valence', 0.03742346418811198),\n",
       " ('emotional', 0.036606816948899494),\n",
       " ('statistic', 0.03611075043157879),\n",
       " ('readability', 0.03574957225478799)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Lemma From Textacy\n",
    "textacy.keyterms.textrank(docx_textacy, normalize='lemma', n_keyterms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('level natural language processing', 0.3056854092725264),\n",
       " ('performance spacy library', 0.1095635417770438),\n",
       " ('textacy', 0.0850709659268793),\n",
       " ('python library', 0.07274216720436877),\n",
       " ('tasks', 0.051788257056594116),\n",
       " ('speech tagging', 0.046870946578303783),\n",
       " ('emotional valence analysis', 0.04081245273285479),\n",
       " ('higher', 0.03519922647193362)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using SRank\n",
    "textacy.keyterms.sgrank(docx_textacy, ngrams=(1, 2, 3, 4), normalize='lower', n_keyterms=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text and Readability Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Counts\n",
    "+ Collections & Counter\n",
    "+ Textacy.TextStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcounts = textacy.TextStats(docx_textacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Unique Words\n",
    "textcounts.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytokens = [ token.text for token in docx_textacy ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Counts \n",
    "textcounts = textacy.TextStats(docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words\n",
    "len(mytokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_sents': 18,\n",
       " 'n_words': 532,\n",
       " 'n_chars': 2756,\n",
       " 'n_syllables': 810,\n",
       " 'n_unique_words': 305,\n",
       " 'n_long_words': 156,\n",
       " 'n_monosyllable_words': 357,\n",
       " 'n_polysyllable_words': 73}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic Counts\n",
    "textcounts.basic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More Specific\n",
    "textcounts.basic_counts['n_sents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collections\n",
    "# Remove Punct,Stop \n",
    "# Nouns\n",
    "from collections import Counter\n",
    "nouns = [ token.text for token in docx if token.is_stop != True and token.is_punct !=True and token.pos_ == 'NOUN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = Counter(nouns)\n",
    "\n",
    "common_nouns = word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 6),\n",
       " ('century', 4),\n",
       " ('capital', 2),\n",
       " ('times', 2),\n",
       " ('millennium', 2),\n",
       " ('period', 2),\n",
       " ('walls', 2),\n",
       " ('population', 2),\n",
       " ('importance', 2),\n",
       " ('plateau', 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag Of Terms\n",
    "+ the Bag-of-words model is term frequency, namely, the number of times a term appears in the text.\n",
    "- Uses\n",
    " - For word or term frequency\n",
    " - For document classification via using it as a feature for training classifier\n",
    " - Computer vision\n",
    " - Similar to N-Grams but no spatial info is preserved unlike n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Terms or Words\n",
    "bot = docx_textacy.to_bag_of_terms(ngrams=(1, 2, 3), named_entities=True, weighting='count',as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('library', 3),\n",
       " ('textacy', 2),\n",
       " ('high', 2),\n",
       " ('task', 2),\n",
       " ('parse', 2),\n",
       " ('nlp', 1),\n",
       " ('spacy', 1),\n",
       " ('python', 1),\n",
       " ('perform', 1),\n",
       " ('level', 1),\n",
       " ('natural', 1),\n",
       " ('language', 1),\n",
       " ('processing', 1),\n",
       " ('build', 1),\n",
       " ('performance', 1)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bot.items(), key=lambda x: x[1], reverse=True)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a Text Document\n",
    "example2 = \"\"\"Ubiquitous, mobile supercomputing. Intelligent robots. Self-driving cars. Neuro-technological brain enhancements. Genetic editing. The evidence of dramatic change is all around us and it’s happening at exponential speed.\n",
    "\n",
    "Professor Klaus Schwab, Founder and Executive Chairman of the World Economic Forum, has been at the centre of global affairs for over four decades. He is convinced that we are at the beginning of a revolution that is fundamentally changing the way we live, work and relate to one another, which he explores in his new book, The Fourth Industrial Revolution.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling with Spacy & Textacy\n",
    "+ A type of statical model used to detect and identify the abstracts and hidden topics in a document\n",
    "+ Uses\n",
    " - Text classification\n",
    " - Discover the latent semantic structures in a document\n",
    " - Organize and Get insight into large collection of data\n",
    " - For Bioinformatics\n",
    " - Recommendation Systems via similarity of topics\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer\n",
    "vectorizer = textacy.Vectorizer(weighting='tfidf', normalize=True, smooth_idf=True,min_df=2, max_df=0.95)\n",
    "doc_term_matrix = vectorizer.fit_transform((doc.to_terms_list(ngrams=1, named_entities=True, as_strings=True)for doc in corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repr(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and interpret a topic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Fit the Model\n",
    "model = textacy.TopicModel('nmf', n_topics=10)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_˓→n=10):\n",
    "    print('topic', topic_idx, ':', ' '.join(top_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jesse JCharis\n",
    "# J-Secur1ty \n",
    "# Jesus Saves @JCharisTech"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
