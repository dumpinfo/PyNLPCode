{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Summarizing news data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3aee62b9-47ce-e416-5816-8df7126fe690"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gzip\n",
    "import codecs\n",
    "import re\n",
    "import time\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.contrib.seq2seq import TrainingHelper, GreedyEmbeddingHelper, BasicDecoder, dynamic_decode\n",
    "from tensorflow.contrib.seq2seq import BahdanauAttention, AttentionWrapper, sequence_loss\n",
    "from tensorflow.contrib.rnn import GRUCell, DropoutWrapper\n",
    "TOKEN_GO = '<GO>'\n",
    "TOKEN_EOS = '<EOS>'\n",
    "TOKEN_PAD = '<PAD>'\n",
    "TOKEN_UNK = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "654175ff-07c7-455d-1b8f-d702cee211c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titledata=[]\n",
    "artdata=[]\n",
    "with gzip.open('data/news.txt.gz') as artfile:\n",
    "    for li in artfile:\n",
    "        artdata.append(li.decode())\n",
    "with gzip.open('data/summary.txt.gz') as titlefile:\n",
    "    for li in titlefile:\n",
    "        titledata.append(li.decode())\n",
    "news = pd.DataFrame({'Text':artdata,'Summary':titledata})\n",
    "news = news.sample(frac=0.5)\n",
    "news['Text_len'] = news.Text.apply(lambda x: len(x.split()))\n",
    "news['Summary_len'] = news.Summary.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news['Text'].head(2).values)\n",
    "print(news['Summary'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48f10124-4c1b-1f09-512d-352c068de1b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_summaries = []\n",
    "for summary in news.Summary:\n",
    "    news_summaries.append(summary)\n",
    "news_texts = []\n",
    "for text in news.Text:\n",
    "    news_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3f980247-3c32-240d-7d3d-7b0c3c6c13e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(words_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in words_dict:\n",
    "                words_dict[word] = 1\n",
    "            else:\n",
    "                words_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3e9ce130-88f4-8779-5b5f-86f2a23ab347"
   },
   "outputs": [],
   "source": [
    "word_counts_dict = {}\n",
    "count_words(word_counts_dict, news_summaries)\n",
    "count_words(word_counts_dict, news_texts)\n",
    "            \n",
    "print(\"Total words in Vocabulary:\", len(word_counts_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_word_vector_matrix(vector_file):\n",
    "    embedding_index = {}\n",
    "    with codecs.open(vector_file, 'r', 'utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            sr = line.split()\n",
    "            if(len(sr)<26):\n",
    "                continue\n",
    "            word = sr[0]\n",
    "            embedding = np.asarray(sr[1:], dtype='float32')\n",
    "            embedding_index[word] = embedding\n",
    "    return embedding_index\n",
    "# Replace the path here to point to the glove.6B.50d.txt vectors file on your system\n",
    "embeddings_index = build_word_vector_matrix('../../temp/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0be42a13-70b2-e9cc-7468-1247b01f109c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2int = {} \n",
    "count_threshold = 20\n",
    "value = 0\n",
    "for word, count in word_counts_dict.items():\n",
    "    if count >= count_threshold or word in embeddings_index:\n",
    "        word2int[word] = value\n",
    "        value += 1\n",
    "\n",
    "\n",
    "special_codes = [TOKEN_UNK,TOKEN_PAD,TOKEN_EOS,TOKEN_GO]   \n",
    "\n",
    "for code in special_codes:\n",
    "    word2int[code] = len(word2int)\n",
    "\n",
    "int2word = {}\n",
    "for word, value in word2int.items():\n",
    "    int2word[value] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4401990d-4baf-3f30-becc-3a6149716b56"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "nwords = len(word2int)\n",
    "\n",
    "word_emb_matrix = np.zeros((nwords, embedding_dim), dtype=np.float32)\n",
    "for word, i in word2int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_emb_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        word_emb_matrix[i] = new_embedding\n",
    "print(\"Length of word embeddings: \", len(word_emb_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25cfd0e3-ae3d-8728-1c82-1a61bb06aa0e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_sentence_to_ids(text, eos=False):\n",
    "    wordints = []\n",
    "    word_count = 0\n",
    "    for sentence in text:\n",
    "        sentence2ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in word2int:\n",
    "                sentence2ints.append(word2int[word])\n",
    "            else:\n",
    "                sentence2ints.append(word2int[TOKEN_UNK])\n",
    "        if eos:\n",
    "            sentence2ints.append(word2int[TOKEN_EOS])\n",
    "        wordints.append(sentence2ints)\n",
    "    return wordints, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "360cfdf4-ad4c-0316-56d3-70b6206e75e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_summaries, word_count = convert_sentence_to_ids(news_summaries)\n",
    "id_texts, word_count = convert_sentence_to_ids(news_texts, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2a0ae7cd-a845-23dc-3563-ad133e2f02b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unknown_tokens(sentence):\n",
    "    unk_token_count = 0\n",
    "    for word in sentence:\n",
    "        if word == word2int[TOKEN_UNK]:\n",
    "            unk_token_count += 1\n",
    "    return unk_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50d631a2-fb5a-bb0d-6155-9cd15e70835b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_summaries_filtered = []\n",
    "news_texts_filtered = []\n",
    "max_text_length = int(news.Text_len.mean() + news.Text_len.std())\n",
    "max_summary_length = int(int(news.Summary_len.mean() + news.Summary_len.std()))\n",
    "min_length = 4\n",
    "unknown_token_text_limit = 10\n",
    "unknown_token_summary_limit = 4\n",
    "\n",
    "for count,text in enumerate(id_texts):\n",
    "    unknown_token_text = unknown_tokens(id_texts[count])\n",
    "    unknown_token_summary = unknown_tokens(id_summaries[count])\n",
    "    text_len = len(id_texts[count])\n",
    "    summary_len = len(id_summaries[count])\n",
    "    if((unknown_token_text>unknown_token_text_limit) or (unknown_token_summary>unknown_token_summary_limit)):\n",
    "        continue\n",
    "    if(text_len<min_length or summary_len<min_length or text_len>max_text_length or summary_len>max_summary_length):\n",
    "        continue\n",
    "    news_summaries_filtered.append(id_summaries[count])\n",
    "    news_texts_filtered.append(id_texts[count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "34d28c5f-8016-6b36-664e-3d5ee3db745d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs_data = tf.placeholder(tf.int32, [None, None], name='input_data')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    dropout_probs = tf.placeholder(tf.float32, name='dropout_probs')\n",
    "    summary_len = tf.placeholder(tf.int32, (None,), name='summary_len')\n",
    "    max_summary_len = tf.reduce_max(summary_len, name='max_summary_len')\n",
    "    text_len = tf.placeholder(tf.int32, (None,), name='text_len')\n",
    "    return inputs_data, targets, learning_rate, dropout_probs, summary_len, max_summary_len, text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9c9b6087-3c28-478d-d311-4213e1c59654",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, word2int, batch_size):\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    decoding_input = tf.concat([tf.fill([batch_size, 1], word2int[TOKEN_GO]), ending], 1)\n",
    "    return decoding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d675562b-a9e0-df71-6979-a052fb78dcbc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cell(csize,dprob):\n",
    "    rnc = GRUCell(csize)\n",
    "    rnc = DropoutWrapper(rnc, input_keep_prob = dprob)\n",
    "    return rnc\n",
    "\n",
    "def encoding_layer(csize, len_s, nl, rinp, dprob):\n",
    "    for l in range(nl):\n",
    "        with tf.variable_scope('encoding_l_{}'.format(l)):\n",
    "            rnn_frnt = get_cell(csize,dprob)\n",
    "            rnn_bkwd = get_cell(csize,dprob)\n",
    "            eop, est = tf.nn.bidirectional_dynamic_rnn(rnn_frnt, rnn_bkwd, \n",
    "                                                                    rinp,\n",
    "                                                                    len_s,\n",
    "                                                                    dtype=tf.float32)\n",
    "    eop = tf.concat(eop,2)\n",
    "    return eop, est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "524d0246-ddae-b485-5ea4-11ad476447f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trng_dec_layer(dec_emb_inp, summ_len, cell_dec, st_init, lyr_op, \n",
    "                            v_size, max_summ_len):\n",
    "    helper = TrainingHelper(inputs=dec_emb_inp,sequence_length=summ_len, time_major=False)\n",
    "    dec = BasicDecoder(cell_dec,helper,st_init,lyr_op) \n",
    "    logits, _, _ = dynamic_decode(dec,output_time_major=False,impute_finished=True, \n",
    "                                  maximum_iterations=max_summ_len)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6044b206-7f27-5304-4896-06d388af0949",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def infr_dec_layer(embeddings, start_token, end_token, decoding_cell, initial_state, op_layer,\n",
    "                             max_summary_len, batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    inf_helper = GreedyEmbeddingHelper(embeddings,start_tokens,end_token)\n",
    "    inf_decoder = BasicDecoder(decoding_cell,inf_helper,initial_state,op_layer)       \n",
    "    inf_logits, _, _ = dynamic_decode(inf_decoder,output_time_major=False,impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_len)\n",
    "    return inf_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4b50746f-8f78-0253-9178-56c62e4ac1bf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_emb_op, embs, enc_op, enc_st, v_size, txt_len, \n",
    "                   summ_len,mx_summ_len, rnsize, word2int, dprob, batch_size, nlyrs):\n",
    "    \n",
    "    for l in range(nlyrs):\n",
    "        with tf.variable_scope('dec_rnn_layer_{}'.format(l)):\n",
    "            gru = tf.contrib.rnn.GRUCell(rnn_len)\n",
    "            cell_dec = tf.contrib.rnn.DropoutWrapper(gru,input_keep_prob = dprob)\n",
    "    out_l = Dense(v_size, kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attention = BahdanauAttention(rnsize, enc_op,txt_len,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "    cell_dec =  AttentionWrapper(cell_dec,attention,rnn_len)\n",
    "    attn_zstate = cell_dec.zero_state(batch_size , tf.float32 )\n",
    "    attn_zstate = attn_zstate.clone(cell_state = enc_st[0])\n",
    "    with tf.variable_scope(\"decoding_layer\"):\n",
    "        tr_dec_op = trng_dec_layer(dec_emb_op, \n",
    "                                                  summ_len, \n",
    "                                                  cell_dec, \n",
    "                                                  attn_zstate,\n",
    "                                                  out_l,\n",
    "                                                  v_size, \n",
    "                                                  mx_summ_len)\n",
    "    with tf.variable_scope(\"decoding_layer\", reuse=True):\n",
    "        inf_dec_op = infr_dec_layer(embs,  \n",
    "                                                    word2int[TOKEN_GO], \n",
    "                                                    word2int[TOKEN_EOS],\n",
    "                                                    cell_dec, \n",
    "                                                    attn_zstate, \n",
    "                                                    out_l,\n",
    "                                                    mx_summ_len,\n",
    "                                                    batch_size)\n",
    "\n",
    "    return tr_dec_op, inf_dec_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19ddcf22-4f6a-d531-071a-021b42b643e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(data_inp, data_summ_tgt, dprob, len_txt, len_summ, max_len_summ, \n",
    "                  v_size, rnsize, nlyrs, word2int, batch_size):\n",
    "    \n",
    "    inp_emb = word_emb_matrix\n",
    "    word_embs = tf.Variable(inp_emb, name=\"word_embs\")\n",
    "    inp_enc_emb = tf.nn.embedding_lookup(word_embs, data_inp)\n",
    "    op_enc, st_enc = encoding_layer(rnsize, len_txt, nlyrs, inp_enc_emb, dprob)\n",
    "    \n",
    "    inp_dec = process_encoding_input(data_summ_tgt, word2int, batch_size)\n",
    "    inp_dec_emb = tf.nn.embedding_lookup(inp_emb, inp_dec)\n",
    "    \n",
    "    op_tr, op_inf  = decoding_layer(inp_dec_emb, \n",
    "                                                        inp_emb,\n",
    "                                                        op_enc,\n",
    "                                                        st_enc, \n",
    "                                                        v_size, \n",
    "                                                        len_txt, \n",
    "                                                        len_summ, \n",
    "                                                        max_len_summ,\n",
    "                                                        rnsize, \n",
    "                                                        word2int, \n",
    "                                                        dprob, \n",
    "                                                        batch_size,\n",
    "                                                        nlyrs)\n",
    "    \n",
    "    return op_tr, op_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "725e92bf-2309-1a78-c771-641a42b440c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_batch):\n",
    "\n",
    "    max_sentence = max([len(sentence) for sentence in sentences_batch])\n",
    "    return [sentence + [word2int[TOKEN_PAD]] * (max_sentence - len(sentence)) for sentence in sentences_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47e4f70a-6377-68dd-c06c-eed674b2bb3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    for batch_idx in range(0, len(texts)//batch_size):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        summaries_batch = summaries[start_idx:start_idx + batch_size]\n",
    "        texts_batch = texts[start_idx:start_idx + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentences(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentences(texts_batch))\n",
    "\n",
    "        pad_summaries_lens = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lens.append(len(summary))\n",
    "        \n",
    "        pad_texts_lens = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lens.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lens, pad_texts_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77299c4b-a3cf-785b-981a-42a1bb3a2033",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 64\n",
    "rnn_len = 256\n",
    "n_layers = 2\n",
    "lr = 0.005\n",
    "dr_prob = 0.75\n",
    "logs_path='/tmp/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68781626-8bf4-0a23-4bb2-f24a5762fa1e"
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    data_inp, tgts, lrt, dprobs, len_summ, max_len_summ, len_txt = model_inputs()\n",
    "\n",
    "    tr_op, inf_op = seq2seq_model(tf.reverse(data_inp, [-1]),\n",
    "                                                      tgts, \n",
    "                                                      dprobs,   \n",
    "                                                      len_txt,\n",
    "                                                      len_summ,\n",
    "                                                      max_len_summ,\n",
    "                                                      len(word2int)+1,\n",
    "                                                      rnn_len, \n",
    "                                                      n_layers, \n",
    "                                                      word2int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    tr_op = tf.identity(tr_op.rnn_output, 'tr_op')\n",
    "    inf_op = tf.identity(inf_op.sample_id, name='predictions')\n",
    "    \n",
    "    seq_masks = tf.sequence_mask(len_summ, max_len_summ, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        tr_cost = sequence_loss(tr_op,tgts,seq_masks)\n",
    "        optzr = tf.train.AdamOptimizer(lrt)\n",
    "        grds = optzr.compute_gradients(tr_cost)\n",
    "        capped_grds = [(tf.clip_by_value(grd, -5., 5.), var) for grd, var in grds \n",
    "                        if grd is not None]\n",
    "        train_op = optzr.apply_gradients(capped_grds)\n",
    "    tf.summary.scalar(\"cost\", tr_cost)\n",
    "print(\"Graph created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6368ba0d-4083-e182-ca38-5356b307e09f"
   },
   "outputs": [],
   "source": [
    "min_learning_rate = 0.0006\n",
    "display_step = 20 \n",
    "early_stop_cnt = 0 \n",
    "early_stop_cnt_max = 3 \n",
    "per_epoch = 3 \n",
    "\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] \n",
    "\n",
    "news_summaries_train = news_summaries_filtered[0:3000]\n",
    "news_texts_train = news_texts_filtered[0:3000]\n",
    "update_check = (len(news_texts_train)//batch_size//per_epoch)-1\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt' \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    tf_summary_writer = tf.summary.FileWriter(logs_path, graph=train_graph)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_len, texts_len) in enumerate(\n",
    "                get_batches(news_summaries_train, news_texts_train, batch_size)):\n",
    "            before = time.time()\n",
    "            _,loss,summary = sess.run(\n",
    "                [train_op, tr_cost,merged_summary_op],\n",
    "                {data_inp: texts_batch,\n",
    "                 tgts: summaries_batch,\n",
    "                 lrt: lr,\n",
    "                 len_summ: summaries_len,\n",
    "                 len_txt: texts_len,\n",
    "                 dprobs: dr_prob})\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            after = time.time()\n",
    "            batch_time = after - before\n",
    "            tf_summary_writer.add_summary(summary, epoch_i * batch_size + batch_i)\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('** Epoch {:>3}/{} Batch {:>4}/{} - Batch Loss: {:>6.3f}, seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(news_texts_filtered) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('Saving model') \n",
    "                    early_stop_cnt = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    early_stop_cnt += 1\n",
    "                    if early_stop_cnt == early_stop_cnt_max:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "\n",
    "        if early_stop_cnt == early_stop_cnt_max:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "989d2498-df4e-31e8-3133-56df524b8a28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    return [word2int.get(word, word2int[TOKEN_UNK]) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "265fd2f2-cd5f-590d-1fa3-cf6eeedf89fe"
   },
   "outputs": [],
   "source": [
    "#random = np.random.randint(3000,len(news_texts_filtered))\n",
    "random = np.random.randint(0,3000)\n",
    "text = news_texts_filtered[random]\n",
    "\n",
    "checkpoint = logs_path + 'best_so_far_model.ckpt'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input_data:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_len:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_len:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('dropout_probs:0')\n",
    "    result_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "pad = word2int[TOKEN_PAD] \n",
    "\n",
    "#print('\\nOriginal Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join( [int2word[i].decode('utf-8') for i in text if type(int2word[i]) is bytes] )))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in result_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join( [int2word[i].decode('utf-8') for i in result_logits\n",
    "                                               if type(int2word[i]) is bytes and i!=pad] )))\n",
    "print(' Ground Truth: {}'.format(\" \".join( [int2word[i].decode('utf-8') for i in news_summaries_filtered[random] \n",
    "                                            if type(int2word[i]) is bytes] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
